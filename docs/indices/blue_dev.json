[
  {
    "objectID": "53b57c8f5b81ea66706ed212cf752dbc",
    "u": "https://appsec.tymyrddin.dev/",
    "t": "Securing web applications ",
    "c": "Securing web applications  The process of designing and reviewing a software design with security considerations in mind. Some notes … Lockdown environment Overview Communication with and between servers Containers Allow locked-down clients of end-users Hosted repositories Coding Overview Authentication Use cache securely File upload Input validation Javascript Local file inclusion (LFI) Output validation Python Arbitrary code execution Remote file inclusion (RFI) XSS mitigations Libraries and frameworks Overview Javascript frameworks npm PyPI Python frameworks Protocols Overview Use TLS/SSL more securely Databases Overview Access control Input validation Parameterised statements API Introduction Real-time monitoring Vulnerability scanning Never trust user data API testing tools Security testing Introduction Code reviews Configuration analyses Database frangibility scanning Architecture and design validation Network vulnerability scanning Web service scanning Source code analysis Web application vulnerability scanning Books  https://nostarch.com/designing-secure-software https://nostarch.com/websecurity https://www.manning.com/books/securing-devops https://bpbonline.com/products/implementing-devsecops-with-docker-and-kubernetes",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "186b18ef3abae1e4847b6516717b7115",
    "u": "https://appsec.tymyrddin.dev/docs/lockdown/readme",
    "t": "Overview ",
    "c": "Overview  Lock down the development and production environment, using available security features to limit the scope of what the software under development can do to its host systems and adjacent networks. Even if an adversary finds and exploits a vulnerability, an environmental lockdown can restrict what the adversary can do, reducing the overall severity of the problem and making the adversary work harder. In some cases, the lockdown may prevent the problem from becoming a vulnerability in the first place. Run as an unprivileged user. Run any software with the lowest privileges possible. When using third-party libraries, use libraries with an established record of good security . Use operating system and hardware features that limit the impact of a successful attack. Use isolation and separation such as sandboxes, docker, or similar, to limit access (and potential damages) to the environment. Adopt secure configurations using security benchmarks or hardening guides. Proactively monitor for updates and patches to servers, scripts, software and applications. Use vulnerability scanners and regularly apply security patches to check that the system does not have any known vulnerabilities. Use some security by obscurity (robots.txt, .htaccess, nginx .conf, etc.) (SQL Injection Attacks) Use an application firewall (WAF) Search the domain for sensitive information, or strings that might not be secure. Dork it. (SQL Injection Attacks) Allow locked-down clients",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "ada3608ac956cbd8154447367fd84e4e",
    "u": "https://appsec.tymyrddin.dev/docs/libraries/pypi",
    "t": "PyPI ",
    "c": "PyPI  Assume that there are malicious packages within PyPI. Do a bit of research on the package you want to install and carefully spell out the package name when installing it (a package named for a common misspelling of a popular package could execute malicious code). Make yourself familiar with the licenses necessary for the projects you use, so you are sure that you are not compromising yourself legally. Scan for security vulnerabilities with Bandit , available through the Python Packaging Index (PyPI) Use Pipenv , a tool for managing the competing interests of having a predictable development environment and having an up-to-date development and production environment. It uses a two-file system that separates abstract dependency declarations from the last tested combination. Even when using Python 3, it is still important to keep in mind that import statements execute the code within the target module. The requests library handles SSL certificate verification. This library uses a package called certifi to validate the trustworthiness of certificate authorities. Maintaining the most updated version of certifi and never ever pin this dependency . Oh, and do not bypass a certificate verification request. Do not deserialise data from an untrusted source, and for PyYAML always use yaml.safe_load . Look into dependency analysis tools. There are some free tools available such as ochrona .",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "14d8f98e9ef989f2991e2316a0055531",
    "u": "https://appsec.tymyrddin.dev/docs/coding/readme",
    "t": "Overview ",
    "c": "Overview  Disable verbose error messages for messages that are displayed to users without special privileges. Use context-sensitive server side output encoding, a combination of escaping, filtering, and validating string data when handling user input and output from the server (cross-site scripting (XSS)) Replace special characters with escape codes for those characters. Remove dangerous characters from the data received as input. This is not enough. There are some techniques adversaries can use to evade such filters. Validate browser-supplied input for it to only contain expected characters. Use whitelisting of acceptable characters and reject everything else. Use client and server-side validation. Python validation can be used for making sure only expected data makes it into the application, and to inform users immediately of issues with their input. Establish and maintain control over all inputs Establish and maintain control over all outputs Mitigate the language specific most common vulnerabilities JavaScript Python",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "6f0685d82533e28cfc282af16ad52e11",
    "u": "https://appsec.tymyrddin.dev/docs/databases/readme",
    "t": "Overview ",
    "c": "Overview  Implement access control schemes Restrict access to database objects and functionality, according to the Principle of The Least Privilege (Insecure direct object references). Base input validation on a whitelist. Use the most restrictive rule by default and allow special characters only by exception. This will reduce the attack surface for many vectors. Use parameterised statements",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "c7259adcfe48016637e950d9f1331245",
    "u": "https://appsec.tymyrddin.dev/docs/coding/file-upload",
    "t": "File upload ",
    "c": "File upload  Never allow users to upload executable files. Use a whitelist of allowed file types. Check file type AND file extension. Verify file type against the whitelist. Use input validation to prevent the whitelist from being bypassed using the filename. Use input validation to prevent the metadata from being exploited. For example, remove any unnecessary metadata such as exif data from images and remove control characters from filenames and extensions. Remove any unnecessary file evaluation. Limit the size of the filename. Limit the size of the file (unexpectedly small files and large files can both be used in denial of service attacks). Limit the directory to which files are uploaded. Scan all files with antivirus software. Name the files randomly or using a hash instead of by the user’s input. This will prevent an adversary from scripting access to uploaded files using the filename as an attack vector. Simplify error messages. Remove any directory paths and server configurations from error messages that adversaries could use. Check the uploaded directory to make sure the read/write/execute user permissions are correct.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "457dbd60d5eb6e880587706c65a5a9f0",
    "u": "https://appsec.tymyrddin.dev/docs/coding/js",
    "t": "Javascript ",
    "c": "Javascript  Avoid eval() . Instead, opt for alternative options that are more secure. Use HTTPS/SSL to encrypt data exchanged between the client and the server. Set cookies as “secure,” limiting the use of the application’s cookies to only secure web pages. Assign individual tokens for each end user. If the tokens do not match up, deny or revoke access (API access keys). Use safe methods of DOM manipulation. innerHTML does not limit or escape/encode values passed on to them. Use innerText instead. It provides escaping (preventing DOM-based XSS attacks).",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "bc33ba64371810efef0ad5b40f1554c8",
    "u": "https://appsec.tymyrddin.dev/docs/lockdown/repository-hosts",
    "t": "Hosted repositories ",
    "c": "Hosted repositories  Set up MFK Use GitHub’s search features as well as scraping tools to check your own code for potential data leaks. Identify, configure and audit production branches to Not allow force pushes. Only give commit privileges to a small set of users. Enforce those restrictions on admins & owners too! Require all commits to be PGP signed (keys known in advance). Read more recommendations by Mozilla (they learned the hard way, no need to repeat that for ourselves). As of November 2017, GitHub tracks reported vulnerabilities in certain dependencies and provides security alerts to affected repositories. Related attack trees  Poison open source supply chains",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "a1bc78eec669e3e9f0a91145ca0fd292",
    "u": "https://appsec.tymyrddin.dev/docs/testing/source-analysis",
    "t": "Source code analysis ",
    "c": "Source code analysis  In some cases, dependent on what the changes are, what tools the development environment contains, in what language is developed and what analysis tools are available, tools can assist with a part of a manual review. Specifically source code security analysers examine source code to detect and report weaknesses that can lead to security vulnerabilities. Tools  DeepSource SonarQube Codacy",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "9c79d8ef842c63aaf1ec1e464daaf817",
    "u": "https://appsec.tymyrddin.dev/docs/libraries/readme",
    "t": "Overview ",
    "c": "Overview  Use libraries and frameworks that make it easier to avoid introducing weaknesses. The vulnerability risk isn’t just the sum of all the unfixed security bugs in the code of a project installed, it includes the recursive sum of all the security bugs in all the sub-projects (libraries and frameworks) on which the application depends.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "1199008010dcb3e69d12cd7ec0f6987c",
    "u": "https://appsec.tymyrddin.dev/docs/lockdown/end-users",
    "t": "Allow locked-down clients of end-users ",
    "c": "Allow locked-down clients of end-users  Some end-users may have Javascript disabled for security, and requiring JS features exposes them to attacks from other websites. If a user has a locked-down client that does not support all the features used by your application, consider allowing that user some access, instead of completely denying access, which may force the user to degrade the client’s security. This may help protect users and slow the propagation of malware.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "5add87a43b187e28a09a60a2400f21e8",
    "u": "https://appsec.tymyrddin.dev/docs/testing/code-review",
    "t": "Code reviews ",
    "c": "Code reviews  Manual code reviews  Code reviews not only improve code quality, in general they drive increased maintainability by Establishing a common yet evolving mental model Building confidence in direction and design alternatives/decisions Enabling more effective funding requests Exposing end-user interaction points Establishing consensus on supported workflows Discovering best practices Code reviews also facilitate increased system effectiveness: Knowing code is going to be reviewed, stimulates looking it over first, and having to explain it to a reviewer, problems that may have been missed before may become apparent. And if something in the code is not immediately clear to the reviewer, this can be taken as an indication that a better name, an additional comment, or a refactoring is required. In addition, a reviewer may spot vulnerabilities, subtle errors, unnecessary code and design flaws, including in nearby code. Pull request reviews  Code reviews done by the entire team take a lot of everyone’s time, and as a result these reviews become few and far between and only a small percentage of the code base will get reviewed. The “another person reviews changes before commit” usually works better, and in the case of a dedicated reviewer may give the reviewer (and if communicated and documented well for the entire team) an impression of used development practices and architecture “as is”. There are some warnings though: When the change is trivial, it may make little sense to have it reviewed. The discipline to still stick to doing it prevents the slippery slope of declaring changes to be “trivial” when they are not. This is not a very good way to review significant changes to the system or the addition of large new components. For phased changes (not too large refactoring) it can work. It can become expensive and may not catch all attack vectors. Not perfect, but is mentioned as having noticeably improved code quality in several projects if: Time is devoted to it Debt is accepted Churn is identified Pedantry (excessive concern with formalism, accuracy, and precision leading to style over substance) is minimised. Review focus  Does it work according to intent (spec/user stories)? Is it tested? Single purpose for a commit? Algorithmic complexity Exception and error handling Exception, class, and variable naming Logging sufficiency and level Long lines and methods Readability",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "463c337f04d62f0325104544838ebce6",
    "u": "https://appsec.tymyrddin.dev/docs/libraries/js-frameworks",
    "t": "Javascript frameworks ",
    "c": "Javascript frameworks  React Security Angular Security Guide Security Best Practices for Express in Production Vue: A security guide exists for version 2 , but I have found none for version 3 yet. Svelte: No security guide found Hence the choice we made for React front-ends in our projects. And a work-in-progress for learning JS and pentesting JS applications.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "eefdb9ba63d43e8975f1b11fd066bc0d",
    "u": "https://appsec.tymyrddin.dev/docs/testing/config-analysis",
    "t": "Configuration analyses ",
    "c": "Configuration analyses  Tools that test configurations for adhering to any possible security policy are either highly specialised for particular software or devices, or are general and basic. It doesn’t hurt and doesn’t take much time. SolarWinds and ManageEngine offer free trials for their commercial network configuration managers The Cisco Config Analysis Tool analyses the configuration files of Cisco devices based on the Cisco Guide to Harden Cisco IOS Devices. WeConfig is a free configuration manager for industrial networks. Its security analysis checks for network vulnerabilities. rConfig is a free configuration management tool that runs on Linux.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "930d992fa83db348efb69c60f5b953fc",
    "u": "https://appsec.tymyrddin.dev/docs/testing/database-scanning",
    "t": "Database frangibility scanning ",
    "c": "Database frangibility scanning  An excellent tool would check passwords, default account vulnerabilities, login hours violations, account permissions, role permissions, unauthorized object owners, remote login and servers, system table permissions, extended stored procedures, cross database ownership chaining, authentication, login attacks, stale login ids, security of admin accounts, excessive admin actions, passwords, password ageing, auditing trail, auditing configuration, buffer overflows in user name and buffer overflows in database link. Don’t think it exists, but every little bit helps. Tools  AppDetective Pro Scuba Database Vulnerability Scanner",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "10d89626c1d1e15763c615bd368c765d",
    "u": "https://appsec.tymyrddin.dev/docs/coding/authentication",
    "t": "Authentication ",
    "c": "Authentication  Authentication provides a way to collect credentials and determine the identity of a user. During authentication with a web application for example, //Web Agents// communicate with a //Policy Server// to determine the proper credentials that must be retrieved from the user who is requesting resources, potentially introducing a larger attack surface. IOW, the strength of the access control model to guard against unauthorised access depends on the robustness of the authentication scheme being used. Carefully implement an authentication mechanism to control which users are allowed to access which data. The keyword here is the “carefully”, so as to introduce as little new holes in the Emmental cheese as possible. Use SSL Always do the authentication process in a https session. Encrypt credentials in rest (when stored on the server) using hashing, MD5, etc. When the server sets a cookie on a browser, the Http-Only attribute can inform the browser not to allow access to the cookie from the DOM. This prevents client-side script-based attacks from accessing the sensitive data stored in the cookies. Use persistent and transient authentication methods (or a hidden field provided on every form) to aid protection against CSRF Tokenise client-server communication with an additional token not stored in cookies. Each form can have a separate token when the session is established and be sent with each request during the session. Implement session tokens and postfix the token in all requests raised by the user (Cross-site request forgery (CSRF)) Do not store sensitive information such as tokens in browser storage. Make the token pseudo-random enough to make it computationally expensive for an attacker to guess. For highly sensitive operations, add a user interaction based protection (re-authentication or a one-time token) along with token based mitigation. Terminate idle sessions (Cross-site request forgery (CSRF)) Sandbox applications so that adversaries will need to use two exploits: one for the vulnerable browser or add-on/extension and another to break out of the sandbox.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "5ce96b937f0fb77c91c71e1f9133b3ee",
    "u": "https://appsec.tymyrddin.dev/docs/api/tools",
    "t": "API testing tools ",
    "c": "API testing tools  Open Source  Astra : A Python-based tool for API Automated Security Testing. Apache JMeter : A Java application designed to load test functional behaviour and measure performance. Designed for testing Web Applications and has since expanded to other test functions. Rest-Assured : Java DSL for easy testing of REST services. Karate Labs : An open source framework for API testing, front-end UI testing, and performance testing. Citrus Framework : An open source framework for Fautomated integration tests. Parasoft SOAtest : A web API automation tool making use of two services, SOAP and REST, for functional, regression, and unit testing, runtime error detection, static code analysis, service virtualisation, etc. Tricentis : “shift left” testing so that defects are found when they are fastest, easiest, and cheapest to fix. Commercial  Katalon Studio : Automation solution for API, Web, and mobile testing - Offers a free (trial) version. SoapUI : End-to-end tests on REST, SOAP, & GraphQL APIs, JMS, JDBC, and other web services Offers a free trial. Postman API testing : APIs and service performance and health monitor - Offers a limited free version. Apigee : A tool for building, managing, and securing APIs - Try it free",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "9adbbf8c38e80ea579da5f1ecb5fa286",
    "u": "https://appsec.tymyrddin.dev/docs/libraries/npm",
    "t": "npm ",
    "c": "npm  Avoid publishing secrets (API keys, passwords or other secrets) to the npm registry. When updating the .gitignore file, also update .npmignore . The ignore files function as blacklist. Instead, use the files property in package.json . It works as a whitelist and specifies the array of files to be included in the package (the ignore file functions as a blacklist). They can be used together to determine which files should explicitly be included and excluded from the package. The files property in package.json takes precedence over the ignore file. Before publishing do a dry run by adding the --dry-run argument to the publish command to review what will be in the tarball. Enforce the lockfile ( yarn install --frozen-lockfile , npm ci ) Reduce attack surface Do not immediately and blindly upgrade to new versions; wait a while (but not until they are outdated of course). Before upgrading, review changelog and release notes. When installing packages, add the --ignore-scripts suffix to disable the execution of scripts by third-party packages. Perhaps add ignore-scripts to the .npmrc project file or the global npm configuration. Maintain project health Run npm outdated to see if any packages are out of date. Run npm doctor to review the npm setup. Check that the official npm registry is reachable and displays the currently configured registry. Check Git is available. Review installed npm and Node.js versions. Run permission checks on the local and global node_modules , and package cache folders. Check the local npm module cache checksum. Audit for vulnerabilities in open source dependencies. Scan with snyk and/or npm audit . See Comparing npm audit with Snyk",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "a85b7f2ce8c437bc2b4f8d85b4cddc14",
    "u": "https://appsec.tymyrddin.dev/docs/testing/readme",
    "t": "Introduction ",
    "c": "Introduction  Security testing often lacks systematic approaches enabling problem-solving oriented identification, selection and execution of test cases. One approach to solving this problem is to use risk-oriented testing, where software risks analysis is used as the guiding factor to solve decision-making problems during testing, in particular selection and prioritisation of test cases. Architecture and design validation can be important for detecting problems that would be too difficult, time-consuming, or expensive to fix after the product has been deployed, but require expert-level participation. Application firewalls and external monitoring/control frameworks (web application firewalls, proxies, IPS, Struts, etc.) are useful mostly for protecting software that is known to contain weaknesses, but customisation may be needed, and not all attacks can be automatically detected or prevented. In addition, configuration analysis tools that test configurations for adhering to security policies for particular software or devices may exist. Likewise, application threat modelling can be useful for finding problems before code is developed and for risk analysis, but it is difficult to teach and requires expert participation. Knowing code is going to be reviewed , stimulates looking it over first, and having to explain it to a reviewer, problems that may have been missed before May become apparent. And if something in the code is not immediately clear to the reviewer, this can be taken as an indication that a better name, an additional comment, or a refactoring is required. And a reviewer may find other problems too, including in nearby code and works well for finding subtle errors and design flaws, and for giving an overall assessment of development practices, but can become expensive and may not catch each and every attack vector. Automated static code analysis for source code or binary code can give a good code coverage for implementation errors, but report a large number of issues that a developer may not wish to address. And design-level problems require human analysis. Automated vulnerability scanning of web application , web service , databases and network with automated tools can give some surprising results saving a lot of time down the road, and do not take much time and effort. Well worth it. Automated dynamic code analysis can find issues that are difficult to detect with static analyses as mentioned above, but code coverage is a problem. Manual dynamic code analysis such as pentesting can be effective for understanding the overall security of the software.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "82cf3a91903cd54934c56b7d1c1a3f80",
    "u": "https://appsec.tymyrddin.dev/docs/databases/input",
    "t": "Input validation ",
    "c": "Input validation  Having an input validation framework is an excellent way to prevent all kinds of zero-day exploits — not just SQL injection. Single quotes or semicolons in a credit card number or numeric user ID are really not needed. Better yet, base validation on a whitelist: accept data fitting a specified structure, instead of rejecting patterns.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "9b64b4e37fc6ddd5aca52212389b52e7",
    "u": "https://appsec.tymyrddin.dev/docs/coding/output",
    "t": "Output validation ",
    "c": "Output validation  Use structured mechanisms that automatically enforce the separation between data and code. These mechanisms may be able to provide the relevant quoting, encoding, and validation automatically, instead of relying on the developer to provide this capability at every point where output is generated. For mitigating the low-hanging fruit, the Top 25 vulnerabilities can be used, except for any “Inclusion of functionality from untrusted control sphere” and “Missing authorisation”. //Understand the context in which the data will be used and the encoding that will be expected. This is especially important when transmitting data between different components, or when generating outputs that can contain multiple encodings at the same time, such as web pages or multi-part mail messages. Study all expected communication protocols and data representations to determine the required encoding strategies.//",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "32e064318be44f0138864abb2ad151dc",
    "u": "https://appsec.tymyrddin.dev/docs/coding/lfi",
    "t": "Local file inclusion (LFI) ",
    "c": "Local file inclusion (LFI)  When trying to detect file inclusion vulnerabilities in code while it is being developed, or before the software is deployed, usually code analysis tools are used. These miss about half of all vulnerabilities and give a lot of false positives, making the process tedious, lengthy and expensive. As a result many developers skip it. Try to avoid passing user-supplied input to the filesystem APIs. Many of the functions that do that can be rewritten to deliver the same behaviour, without the exposure. Validate user input before processing it. Either compare the input against a whitelist of permitted values or verify that the input contains only permitted content – for example, alphanumeric characters. After validation, verify that the canonicalised path starts with the expected base directory. Save file paths in a secure database and give an ID for every single one, this way users only get to see their ID without viewing or altering the path. Use verified and secured whitelist files and ignore everything else. Do not include files on a web server that can be compromised, use a database instead. Make the server send download headers automatically instead of executing files in a specified directory. Do not use dynamic file inclusion. Use static file inclusion.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "ab731d7805e64c672dd393b2d9262101",
    "u": "https://appsec.tymyrddin.dev/docs/testing/design-validation",
    "t": "Architecture and design validation ",
    "c": "Architecture and design validation  Tools that test design decisions for adhering to some basic security policies do not exist (yet) because there are no hard rules or silver bullets regarding what security concerns should be considered for a particular application, but some general secure design principles can help (and some of these are trade-offs with one or some of the other principles): Minimise attack surface – Reduce the number and size of entry points that can be exploited by adversaries as much as possible Principle of least privilege – Give just enough access level to do a job Separate duties – Different entities have different roles Defence in depth – Multiple layers of control make it harder to exploit a system Fail secure – Limit the amount of information that is exposed when a system encounters errors Economy of mechanisms – Keep things as simple as possible Complete mediation – Validate all access to all resources of a system, always Open design – Use proven open standards Psychological acceptability – Protect a system but do not hamper users of the system Weakest link – Any system is only as strong as its weakest link Single point of failure – Add redundancy to critical systems",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "5bba63969c46df17b89832bb73556572",
    "u": "https://appsec.tymyrddin.dev/docs/protocols/readme",
    "t": "Overview ",
    "c": "Overview  Use strict data encryption with a proven encryption technique Use protocol algorithms that are currently considered to be strong by experts in the field, and select well-tested implementations. For countering spoofing and session hijacking: Use Transport Layer Security (TLS), Secure Shell (SSH), HTTP Secure (HTTPS) and other secure communications protocols that encrypt data before it is sent and authenticate data as it is received. Use additional multistep authentication methods. Weaknesses in password hashing algorithms can be used to steal sensitive information stored on a web or application server. Use cryptographic hash functions to implement password hashing. Use pentesting. Even the best senior programmers are susceptible to making mistakes. Review the application for potential vulnerabilities. For best results, do it regularly.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "3077bcacf78ca435a1dccfb5196b59f3",
    "u": "https://appsec.tymyrddin.dev/docs/testing/webapp-scanning",
    "t": "Web application vulnerability scanning ",
    "c": "Web application vulnerability scanning  A web application scanner explores a web application by crawling through its web pages and examines it for security vulnerabilities by generating malicious inputs and evaluating application responses. Tools  OWASP Zed can help find security vulnerabilities in web applications while developing and testing applications. Nikto",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "6ee26db695503da129e9c689ed833318",
    "u": "https://appsec.tymyrddin.dev/docs/coding/rce",
    "t": "Arbitrary code execution ",
    "c": "Arbitrary code execution  Imposing strict control of outgoing connections is only possible for very specialized servers. And even then, there is nothing to stop an adversary from opening a listener on a common port such as 80. All connections would have to be monitored for content. Disabling most tools that make it possible to create a reverse shell is also only possible for specialized servers. Reverse shells can be created using different tools and languages. This would make it more difficult for the adversary, but not impossible. Avoiding or dangerous functions such as Python eval() or passthru() and input validation of dangerous functions (if they have to be used) may help some. Even if succeeding in avoiding reverse shells, there are other methods that an adversary can use to establish control over a system. For example, by using web shells. And reverse shells are always a result of some other kind of attack because the system needs to be compromised first. The best way to avoid reverse shells is to protect against attacks that allow impostors to gain shell access in the first place.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "e3408f37233fcd06e0351d1a9a0ecd57",
    "u": "https://appsec.tymyrddin.dev/docs/coding/input",
    "t": "Input validation ",
    "c": "Input validation  Validate the user input before processing it. Compare against a whitelist of permitted values. If that is not possible, then verify that the input contains only permitted content (for example only alphanumerics) After validation, append the input to the base directory and use a platform filesystem API to canonicalize the path. Verify the canonicalized path starts with the expected base directory. Use a standard input validation mechanism to validate input for length, type of input, syntax, missing or extra inputs, and consistency across related fields. Understand all the potential areas where untrusted inputs can enter the application: parameters or arguments, cookies, anything read from the network, environment variables, reverse DNS lookups, query results, request headers, URL components, e-mail, files, databases, and any external systems that provide data to the application. Inputs may be obtained indirectly through API calls. Validate on the server side to protect against attacks. Server side validation is also important for compatibility. Validate on the client side to give better feedback to users providing input. Some validations can’t be properly done in server-side application code, and are impossible in client-side code, because they depend on the current state of a database. Only the database can reliably validate data which depends on related data. For mitigating the low-hanging fruit, the OWASP has created a list of Top 25 vulnerabilities that can be helpful. MITRE has more detailed descriptions and (coding) examples.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "5d63ee2a39102cdcc555a55435fbdcf5",
    "u": "https://appsec.tymyrddin.dev/docs/lockdown/communications",
    "t": "Communication with and between servers ",
    "c": "Communication with and between servers  Isolate communication between servers in the same account or team within the same region. Only services that are meant to be accessible by clients on the public internet need to be exposed on the public network. Making connections  With a VPN a private network can be mapped that only selected servers can see. Other applications can be configured to pass their traffic over the virtual interface that the VPN software exposes. With some server hosting services, Docker allows for setting up encrypted channels between containers, even when on other servers (within the local network). SSH tunnels are useful for allowing outside access to internal network resources and a great way to protect against connections being sniffed by adversaries. Sysadmins will need a way to get in if and when an ssh server fails. A minimal VNC with X.509 that combines standard VNC authentication with GNUTLS encryption and server identification can be set up. In some countries, proving that you connected to a particular server is enough to be prosecuted (or just arrested and questioned), but SSH doesn’t provide a native way to obfuscate to whom it connects. For that, a Tor proxy server can be set up and clients can SSH over the Tor Onion Service. Further considerations  Each server in an infrastructure can be configured to trust a centralised certificate authority. Any certificate that the authority signs is then implicitly trusted. If all applications and protocols support TLS/SSL encryption, the system can be encrypted without the overhead of a VPN tunnel (which also often uses SSL internally), at the expense of the non-trivial effort of configuring a certificate authority and setting up the rest of the public key infrastructure and managing the certificates (creating, signing, or revoking). Either way will probably work well until PKI (a system that is designed to create, manage, and validate certificates for identifying individuals and encrypting communication) is worth the extra effort (and associated costs). SSL or TLS certificates can be used to authenticate different entities to one another. After authentication, they can also be used to establish encrypted communication. This can prevent MITM attacks where an attacker imitates a server in an infrastructure to intercept traffic. E2EE is not sacred. Protection from SSL Spoofing is required.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "3d22c996c3b16cafad4d6ffb97c33cea",
    "u": "https://appsec.tymyrddin.dev/docs/lockdown/containers",
    "t": "Containers ",
    "c": "Containers  Compromised containers could not only corrupt the processes and tasks that the container is undertaking, it could also open the door for a cascade of additional attacks against the underlying network. As much security by design as possible (before deployment) Monitor applications in runtime Scan everywhere Articles  Container Security for development teams - fresh for 2022",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "201706ee4fd0fe606a012ce54818d36f",
    "u": "https://appsec.tymyrddin.dev/docs/coding/rfi",
    "t": "Remote file inclusion (RFI) ",
    "c": "Remote file inclusion (RFI)  Scanning to detect RFI takes less time and resources than detecting such vulnerabilities through manual penetration testing. Input validation is a much less effective method in this case because adversaries can go around it using clever tricks. Never trust user input. If files must be included in the website or web application code, use a whitelist of allowed file names and locations. In the case of PHP applications, most current installations are configured with allow_url_include set to off in php.ini . This makes it impossible for malicious users to include remote files. However, Local File Inclusion (LFI) is still possible in such a case.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "acdeec7fa2f6dbc45c290ead79841bb8",
    "u": "https://appsec.tymyrddin.dev/docs/databases/parameterisation",
    "t": "Parameterised statements ",
    "c": "Parameterised statements  A specific SQL injection software defense is the use of parameterised statements: Give me parameterised SQL, or give me death . Instead of concatenating strings, use secure database components such as stored procedures, parametrised queries, and object bindings for commands, or an Object-Relational Mapping (ORM) library. A prepared statement is a reference to a pre-interpreted query routine on the database, ready to accept parameters A parametrised query is a query made by the code in such a way that values are passed in alongside some SQL that has placeholder values, like ? or %s . For a parameterised query to be effective in preventing SQL injection, the string that is used in the query must always be a hard-coded constant, and is not to contain any variable data from any origin. Do not be tempted to decide this in a case-by-case manner whether an item of data can be trusted, and continue using string concatenation within the query for cases that are considered safe. It is all too easy to make a mistakes about possible origin, or for changes in other code to violate assumptions about what data is tainted. Many database engines have concepts of a prepared statement that can be constructed explicitly with plaintext query syntax, then reused over the lifetime of a client’s session. Sometimes the query plan can be cached . Parametrised queries do not always use a prepared statement under the hood; they should do so if possible, but sometimes it is just formatting in the parameter values. The difference between a prepared statement and a parametrised query is the shape of the API used. Articles  Python MySQL Execute Parameterized Query using Prepared Statement MySQLJS Performing queries",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "875e49346713faf2a540692b65898504",
    "u": "https://appsec.tymyrddin.dev/docs/protocols/tls-ssl",
    "t": "Use TLS/SSL more securely ",
    "c": "Use TLS/SSL more securely  Preventing MitM  Use HTTPS. Use preloaded HSTS. Harden SSL/TLS ciphers. Implement forward secrecy Use authenticated encryption Disable legacy protocols Generate a secure SSL configuration Historical notes  Do not use SSL 3.0 on a server (this will be a problem if Internet Explorer 6.0 must still be supported). Use TLS 1.1 or TLS 1.2 Most current browsers/servers use ‘’TLS_FALLBACK_SCSV’’. If a client requests a TLS protocol version that is lower than the highest supported by the server (and client), the server will treat it as an intentional downgrade and drop the connection. Do not accept an incorrect padding structure after decryption. Disable HTTP compression. Separate secrets from user input. Randomize secrets per request. Mask secrets (randomize by XORing with a random secret per request). Protect pages against injections. Hide length (for example by adding random numbers of bytes to responses). Limit the rate of requests. The NULL cipher suites inform the browser not to encrypt data, therefore nullifying any protection given through the use of SSL/TLS. Do not have NULL. Disable RC2 and SHA-1. Consider disabling RC4 (Bar Mitzvah), DES and 3DES (Sweet32).",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "bb100763f244a8fd0652ea818466c94f",
    "u": "https://appsec.tymyrddin.dev/docs/coding/cache",
    "t": "Use cache securely ",
    "c": "Use cache securely  Web cache poisoning vulnerabilities come into play due to general flaws in the design of caches and/or its implementation in an application. Turn caching off (OR) If you have to use it (big site, many visitors) Understand and restrict where caching is done. Cache static response, such as .js , .css , .png files, blog posts, or any page that is always identical. Make sure that an adversary can not trick the back-end server into retrieving their malicious version of a static resource. Disable all headers which are not necessary for the website’s functionality. Especially the Host header. Double-check whether which urls really need to be absolute. Instead use relative urls. Do not exclude something from the cache key, rewrite the request. Avoid using user inputs (HTTP Headers) to be used as the cache key. Do not accept fat GET requests. Some third-party technologies permit such fat GET requests by default. In general, disable caching by third party frameworks. Do caching at one point. Patch client-side XSS vulnerabilities. Including those that seem not exploitable. They can become exploitable due to strange behaviours of the cache, so that even in the event of such a vulnerability, the user’s browser can’t be exploited.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "a127c0f3a679c0d18b2d7fdd5df436bc",
    "u": "https://devsecops.tymyrddin.dev/",
    "t": "DevSecOps ",
    "c": "DevSecOps  Shifting left is a buzzword used for conveying that making security an integral part of development is the only practical approach for agile workflows. It’s mission: Find and prevent defects early in the software delivery process. The idea is to improve quality by moving tasks from “the right” to “the left” as early in the software development lifecycle (SDLC) as possible. The awesome assumption is that there is application security testing on “the right”, in staging and production. And a misunderstanding with fatal consequences waits around the corner too. Moving all security testing to development on “the left”, can leave big holes in security on “the right”. To fully incorporate security into a SDLC, there needs to be a mature SDLC process in the first place. An application security program can only be as advanced as the SDLC pipeline itself. Testlab Cloud tools Notes Introduction The story sofar Growing list of challenges SSDLC methodologies Implementing SSDLC Risk assessment Privacy Impact Assessment (PIA) Threat modelling Secure coding Security-testing plan and practices Security automation Shared responsibility Securing virtual machines Securing managed database services Securing containers Securing serverless/function as a service Securing object storage Securing block storage Securing file storage Securing the container storage interface Securing virtual networking Securing DNS services Securing CDN services Securing VPN services Securing DDoS protection services Securing WAF services Identity management Monitoring and auditing Labs AWS Well-architected Labs: Security Microsoft Azure Well-Architected Framework - Security Google cloud Security Engineer Learning Path CloudAcademy Security Training Library Set up labs for trainings Best practices  CI/CD Introduction Docker Code and Git Artifacts AWS Introduction Elastic Compute Cloud (EC2) RDS for MySQL Elastic Container Service (ECS) Elastic Kubernetes Service (EKS) AWS Lambda Simple Storage Service (S3) Elastic Block Store (EBS) Elastic File System (EFS) Container Storage Interface (CSI) Virtual Private Cloud (VPC) Route 53 CloudFront Site-to-Site VPN Client VPN Shield AWS WAF AWS IAM Directory Service Configuring MFA Azure Introduction Virtual Machines Database for MySQL Container Instances (ACI) Kubernetes Service (AKS) Functions Blob storage Managed disks Files Container Storage Interface (CSI) Virtual Network (VNet) Managed DNS Content delivery network (CDN) Site-to-Site VPN Point-to-Site VPN DDoS Protection WAF Azure AD GCP Introduction Compute Engine (GCE) and VM instances SQL for MySQL Kubernetes Engine (GKE) Functions Storage Persistent Disk Filestore Container Storage Interface (CSI) Virtual Private Cloud (VPC) Managed DNS Content delivery network (CDN) Managed VPN Armor Google Cloud IAM Books ",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "0616e8ba68c0c309923f30db8574099a",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/dns",
    "t": "Managed DNS ",
    "c": "Managed DNS  Best practices  Grant minimal permissions for accessing and managing the Azure DNS using Azure role-based access controls (RBACs). Remove unassigned DNS records from your hosted zones (records of resources such as IP addresses that connected to a resource that was removed). Enable the ReadOnly lock for any hosted zone you manage using Azure DNS to protect from accidental changes to DNS records. Use private DNS zones to manage DNS records for internal resources (such as resources located inside private subnets). Use Azure Defender for DNS to detect and send alerts about suspicious DNS-related activities. Enable DNS logging and forward the logs to Azure Sentinel to detect suspicious behaviour on the Azure DNS service.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "d9db1d68707d68d1b47326a6640f25a4",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/armor",
    "t": "Armor ",
    "c": "Armor  Google Cloud Armor is the Google managed DDoS protection and Web Application Firewall (WAF) service. It comes in two price models: Google Cloud Armor Standard: This provides protection against volumetric DDoS attacks and includes preconfigured WAF rules. Managed Protection Plus: This provides protection against volumetric DDoS attacks, includes preconfigured WAF rules, and includes an adaptive protection mechanism. This price tier also offers support from the Google DDoS response team. Cloud Armor protects applications located behind an external Google Cloud load balancer, which is based on HTTP/HTTPS and TCP/SSL proxy load balancers. Best practices  Create an IAM group, add users to the group, and grant the required permissions on the Cloud Armor for the target group. Note that Cloud Armor contains pre-configured protection rules against common web application attacks. If you need to configure your own rules to match your application, create custom Cloud Armor security policies to allow or deny traffic to your applications behind an external Google Cloud load balancer. Enable Cloud Armor logs for any backend service to detect allowed or denied HTTP/HTTPS requests. For critical production applications, it is recommended to subscribe to the Managed Protection Plus service. For critical production applications, it is recommended to enable the Cloud Armor Adaptive Protection Mechanism to detect anomalous activity and generate a custom signature for blocking application attacks using WAF rules (for application attacks). For scenarios where you would like to allow access to your production applications for third-party partners without them having to specify their external IP address or IP range, use Cloud Armor named IP address lists to configure a whitelist of allowed sources. Use the Google Security Command Center service to detect anomalous behaviour in the Cloud Armor traffic activity (such as spikes in traffic).",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "518a35925643e47d11dc8eaf865b0a68",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/sec-assessment",
    "t": "Security-testing plan and practices ",
    "c": "Security-testing plan and practices  A security assessment plays a primary role in achieving security in SDLC and should be implemented in all phases where possible. Security testing assesses a system, software or web application for vulnerabilities and other attack vectors. Because they test from a holistic point of view of the application, they are usually carried out at the end of the SDLC, in the Operations and Maintenance phase, once the version has included all the working components and updates. There are two types of assessments: Penetration Testing and Vulnerability Assessment. Usually, a company employs and authorises external security testers to attempt to break into a company’s network and systems legally. Vulnerability assessment  Vulnerability Assessments focus on finding vulnerabilities , but do not validate them or simulate the findings to prove they are exploitable in reality. Typically, automated tools run against an organisation’s network and systems. Examples of tools: are OpenVAS, Nessus (Tenable), and ISS Scanner. These scanners probe ports and services on systems across various systems and IP Addresses. Other activities include checking service versions against a database of vulnerabilities affecting said version. The result is a report with a list of vulnerabilities usually found, with an automated threat level severity classification, e.g., High/Medium/Low or an assigned CVSS score. Penetration testing  Pentesting includes Vulnerability Testing but goes more in-depth. It is extended by testing/validating of vulnerabilities, quantifying risks and attempting to penetrate systems. For example, trying to escalate privileges after a vulnerability is found, some vulnerabilities can be a lower risk but can be used as leverage to cause more damage. The tester can provide a thorough report with suggested countermeasures to mitigate the vulnerabilities. This makes it easier to understand the threats by demonstrating the actual risk, for example, recovering an employee password by exploiting the mentioned vulnerability. Pros and Cons  Vulnerability assessment  Pros Cons Suitable for quickly identifying potential vulnerabilities Can produce a large number of reports Part of the Penetration Test Quality depends on the tool used Better for Budget, cheaper than Pentests Real-life scenarios for vulnerabilities are not considered (it could be behind a proxy or only exploitable with social engineering/credentials) The low-risk vulnerability may be used as part of a more powerful attack. Penetration testing  Pros Cons Tester shows organisations what an attacker could do. Very Expensive How any vulnerabilities could be used against it by attackers – the real risk Requires extensive planning and time to carry out the tests Can be shown to the customer Secure code review & analysis  According to research conducted by Verizon in 2020 on Data Breaches, 43% of breaches were attacks on web applications, while some other security breaches resulted from some vulnerabilities in web applications. Implementing a secure code review in the phases of an SDLC, especially during the implementation phase, will increase the resilience and security of the product without bearing any additional cost for future patches. Secure code review is defined as a measure where the code itself is verified and validated to ensure vulnerabilities that are found can be mitigated and removed to avoid vulnerabilities and flaws. Reviewing code is a crucial step in the SDLC for developers. It prevents any setbacks on the release and issues exposed to the users. The cost of the project itself and the effort put in is proportional and cheaper in the long run than the cost of applying code reviews and analysis. By implementing this approach, the organisation will also be compliant with the standards set by government bodies and certifications. Code review can be done manually or automated. A manual code review is where an expert analyses and checks the source code by going line by line to identify vulnerabilities. Therefore, a high-quality manual code review requires the expert to communicate with the software developers to get hold of the purpose and functionalities of the application. The analysis output will then be reported to the developers if there is a need for bug fixing. Code analysis  Static analysis examines the source code without executing the program. In contrast, Dynamic analysis looks at the source code when the program is running, static analysis detects bugs at the implementation level, and dynamic analysis detects errors during program runtime. Automated Static Application Security Testing (SAST) automatically analyses and checks the source code. SAST  SAST means Static Application Security Testing, a white box testing method that directly analyses the source code. Many people tend to develop an application that could automate or execute processes quickly and improve performance and user experience, thereby forgetting the negative impact an application that lacks security could cause. Because the test is done before an application is live and running. SAST can even help detect vulnerabilities in an application before the code is merged or integrated into the software if added as part of the SDLC development phase. SCA  Another type of testing goes hand in hand with SAST, Software Composition Analysis (SCA). SCA is used to scan dependencies for security vulnerabilities, helping development teams track and analyse any open-source component brought into a project. SCA is now an essential pillar in security testing as modern applications are increasingly composed of open-source code. Nowadays, one of the biggest challenges developer teams have is ensuring their codebase is secure as applications are assembled from different building blocks. DAST  DAST means Dynamic Application Security Testing, a black-box testing method that finds vulnerabilities at runtime. DAST is a tool to scan any web application to find security vulnerabilities. This tool is used to detect vulnerabilities inside a web application that has been deployed to production. DAST tools will always send alerts to the security team assigned for immediate remediation. DAST works by simulating automated attacks on an application, mimicking a malicious attacker. The goal is to find unexpected outcomes or results that attackers could use to compromise an application. Since DAST tools don’t have internal information about the application or the source code, they attack just as an external hacker would—with the same limited knowledge and information about the application. DAST is a tool that can be integrated very early into the software development lifecycle. Its focus is to help organisations reduce and protect against the risk that application vulnerabilities could cause. It is very different from SAST because DAST uses the Black Box Testing Methodology; it conducts its vulnerability assessment outside as it does not have access to the application source code. DAST is typically used during the testing phase of SDLC. IAST  IAST means Interactive Application Security Testing that analyses code for security vulnerabilities while the app is running. It is usually deployed side by side with the main application on the application server. IAST is an application security tool designed for web and mobile applications to detect and report issues even while running. Before someone can fully comprehend IAST’s understanding, the person must know what SAST and DAST mean. IAST was developed to stop all the limitations in both SAST and DAST. It uses the Grey Box Testing Methodology. IAST testing occurs in real-time, just like DAST, while the application runs in the staging environment. IAST can identify the line of code causing security issues and quickly inform the developer for immediate remediation. IAST checks the source code similar to SAST, but at the post-build stage, unlike SAST, which occurs during...",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "0aaa3f8a6378a1d33465642eaf8267a7",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/s2s-vpn",
    "t": "Site-to-Site VPN ",
    "c": "Site-to-Site VPN  Azure VPN Gateway (Site-to-Site) is a managed service that allows connecting corporate networks to the Azure environment in a secure channel. Best practices  Restrict access to Azure resources inside your Azure environment using NSGs. Use the GCMAES256 algorithm for both encryption of the IPsec tunnel and ensuring the integrity of the traffic passing through the tunnel. Use pre-shared keys to authenticate to the site-to-site VPN tunnel. For large-scale environments with multiple Azure subscriptions and multiple site-to-site VPN gateways, use Azure Firewall to centrally create, enforce, and log network policies across multiple subscriptions. Use Azure Monitor to monitor the VPN tunnels (for example, it could send alerts when the amount of traffic in bytes is above a pre-defined threshold). Enable Azure DDoS Protection to protect your VPN gateway from DDoS attacks.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "d092193158f2978e2db9e4e65335e43f",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/vpc",
    "t": "Virtual Private Cloud (VPC) ",
    "c": "Virtual Private Cloud (VPC)  Network access  To protect resources inside a VPC, Google supports VPC firewall rules – a stateful mechanism for protecting access to resources. You need to configure either inbound and outbound rules, and for each rule, you can configure either an action of allow or deny. Best practices  Create subnets according to the resource function (for example, public subnets for web servers, private subnets for database servers, and so on). For remote access protocols (SSH/RDP), limit the source IP address (or CIDR) to well-known sources. For file sharing protocols (CIFS/SMB/FTP), limit the source IP address (or CIDR) to well-known sources. Use VPC firewall rules to control access between public resources (such as load balancers or publicly facing web servers) and private resources (such as databases) and limit the access to the minimum required ports/protocols. Set names and descriptions for VPC firewall rules to allow a better understanding of any firewall rule’s purpose. Use tagging (also known as labeling) for VPC firewall rules to allow a better understanding of which VPC firewall rule belongs to which VPC resources. Use network tags to configure rules to groups of resources (such as a group of compute engine instances) instead of using IP addresses. To allow outbound access from internal resources inside private subnets to destinations on the internet, use a Cloud NAT gateway. For large-scale environments with multiple Google Cloud VPC projects, use VPC Service Controls to enforce access restrictions over your VPC resources, based on the identity of the IP address. Monitoring  Google allows for monitoring Google Cloud VPC using Google Cloud Logging and VPC Flow Logs. Best practices  Enable VPC audit logs to monitor your VPC components’ activity and the traffic between your VPC resources. Note that admin activity audit logs are enabled by default and cannot be disabled. Explicitly enable data access audit logs to log activities in your Google Cloud VPC. Limit access to audit logs to the minimum number of employees (to avoid unwanted changes to the audit logs). Enable Firewall Rules Logging to audit the functionality of your VPC firewall rules. Enable VPC Flow Logs to log and further analyze allowed and denied traffic activity. In case you need to troubleshoot network issues by capturing network traffic, use Packet Mirroring to copy live network traffic from a compute engine VM instance to an instance group behind an internal load balancer. For large-scale production environments, enable VPC Flow Logs only for short periods of time, for troubleshooting purposes only (due to the high storage cost and large amounts of data generated by VPC Flow Logs). For large-scale production environments, enable Packet Mirroring only for short periods of time, for troubleshooting purposes only (due to high performance impact on the target VM).",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "c9f09276936a2bcf1bfea52b30020466",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/readme",
    "t": "Introduction ",
    "c": "Introduction  What?  DevOps can mean many different things, depending on which part of information technology (IT) it is being applied to. Here focus is on cloud services and the development and operations of web applications. Why?  To be able to support the desperate better. How?  The story sofar Challenges SSDLC methodologies Implementing SSDLC Risk assessment Privacy Impact Assessment (PIA) Threat modelling Secure coding Security assessments Security automation Shared responsibility Securing virtual machines Securing managed database services Securing containers Securing serverless/function as a service Securing object storage Securing block storage Securing file storage Securing the container storage interface Securing virtual networking Securing DNS services Securing CDN services Securing VPN services Securing DDoS protection services Securing WAF services Identity management Monitoring and auditing",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "0c40190eefa0e26dc314c9215acf0362",
    "u": "https://devsecops.tymyrddin.dev/docs/cicd/git",
    "t": "Code and Git ",
    "c": "Code and Git  Clean Git  Removing all secrets from the current state of the repository is not enough. Sensitive information that made it into the Git history can still be accessible to attackers if a Git repository is exposed. Tools like Trufflehog and GitLeaks scan the repository Git history for traces of secrets that the team may have added in the past. And GitOops! is a tool to help attackers and defenders identify lateral movement and privilege escalation paths in GitHub organizations by abusing CI/CD pipelines and GitHub access controls. Linting  Linting is a process performed by tools that identify coding style deviations and unsafe practices, such as: Indexing beyond arrays. Dereferencing null pointers. (Potentially) dangerous data type combinations. Unreachable code. Non-portable constructs. linters are more valuable for interpreted languages such as Python and JavaScript, because there is no compiler to detect errors during development time. And there is “too much”. Implementing a linter on a mature project can become a tremendous task and annoyance. Linters focused on Security: Brakeman for Ruby Bandit for Python Node Security for JavaScript Static Application Security Testing (SAST)  SAST techniques can look through the application in a commit and analyse its dependencies. SAST tools can find buffer overflows, SQL injection flaws, and other issues. If dependencies contain any issues or known security vulnerabilities, the commit will be marked as insecure and will not proceed to deployment. Dynamic Application Security Testing (DAST)  Have DAST techniques spin up a copy of the production environment inside the CI job to scan the containers and executables. The dynamic aspect helps the system catch dependencies that are being loaded at launch time. Data security procedures  Teams need real data to ensure that everything will work smoothly once in production, but the nonproduction systems are usually not secure enough. Update development and testing environments with data pulled from production environments. Then use this recently pulled data to validate features and test experiences and employ data masking to obscure personally identifiable information and other data subject to data compliance requirements. Other techniques include using synthetic data and service virtualisation. Zero-trust principles  Some basic practices include the least-privilege principle, hiding API keys, defining project- and role-based security credentials in CI/CD tools, and securing access for remote devops team members. Automation  Intelligent automation addresses many of the core requirements for successful software delivery. Basic process automation can increase devops productivity by automating routine manual tasks through code. Building on that, work with operational teams and tools to respond to incidents and recognise when technical debt is becoming an operational or security concern. AIops tools centralise operational data, correlate alerts into incidents, and help automate incident response around performance and reliability issues. Security automation protects against threats and attacks while enabling automations that set permissions, patch systems, and respond to security incidents. Many CI/CD tools provide two-way integrations with AIops, security automation, and other generalised IT automation tools. Trigger notifications to these tools as part of the CI/CD pipeline to inform operations and infosec about code deliveries. And vv. Also allow IT ops and infosec automations to trigger builds or rollbacks to support operational and security needs. Resources  OWASP Source Code Analysis Tools Splunk: What Is Artificial Intelligence for IT Operations (AIOps)?",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "59607f9608451aea3e5e49213554e132",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/cdn",
    "t": "Content delivery network (CDN) ",
    "c": "Content delivery network (CDN)  Best practices  Restrict access to origin servers (where your original content is stored) from CDN segments only (allow traffic only from the CDN segments towards servers or services that store content). Prefer sharing content via the HTTPS protocol to preserve the confidentiality of the content and to ensure the authenticity of the content. When distributing content over HTTPS, prefer using TLS 1.2 over older protocols such as SSL v3. Enable Azure CDN logs for audit logging purposes. Forward the logs to Azure Security Center for further investigation. Forward Azure CDN logs to Azure Sentinel (the Azure managed SIEM service) for threat detection.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "95d015051412223e6fc5bbdf32fbda7e",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/route53",
    "t": "Route 53 ",
    "c": "Route 53  Best practices  Create an Identity and Access Management (IAM) group, add users to the group, and grant the required permissions on the Route 53 service for the target group. Enable Domain Name System Security Extensions (DNSSEC signing) on any public-hosted zone to protect against DNS spoofing attacks. Use a new customer master key (CMK) to sign any newly created public-hosted zone. Make sure privacy protection is enabled for any domain you manage using Route 53 to protect the privacy of domain owners’ contact information. Enable the Route 53 domain transfer lock to prevent your domains from being transferred to another registrar. Create a sender policy framework (SPF) record on your Route 53 hosted domain to publicly specify which mail servers are authorised to send emails on behalf of your email domain. Use the Route 53 Resolver DNS Firewall to block DNS-level threats originating from your VPC. Remove unassigned DNS records from your hosted zones (records of resources such as IP addresses that connected to a resource that was removed). Use private hosted zones to manage DNS records for internal resources (such as resources located inside private subnets). Enable public DNS query logging to be able to analyze which public DNS queries were submitted to Route 53 about your domains. Enable Resolver query logging to be able to analyze information such as the Route 53 Resolver DNS Firewall block rules. Enable Amazon GuardDuty to analyze DNS logs and raise alerts about suspicious activity, such as C&C activity, Bitcoin mining, and more.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "cedb731e7f5cad51fce3c9ff1ffbd706",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/csi",
    "t": "Container Storage Interface (CSI) ",
    "c": "Container Storage Interface (CSI)  Amazon Elastic Kubernetes Service (EKS) has a CSI driver for the following storage types: Block storage: EBS Managed NFS: EFS Parallel filesystem (for HPC workloads): Amazon FSx for Lustre Best practices  When creating an IAM policy to connect to a CSI driver, specify the storage resource name instead of using wildcard. Use IAM roles for service accounts to restrict access to your pod. Always use the latest CSI version for your chosen storage type. When using the CSI driver for EBS and its snapshots, always set (in the YAML configuration file) the value of encrypted to True and specify the Amazon KMS key ID (KmsKeyId). This allows the CSI driver to use a key from Amazon KMS. When using the CSI driver for EFS, always set (in the YAML configuration file) the value of encryptInTransit to True. Use Amazon Secrets Manager with the secret store CSI driver to store and retrieve secrets (such as tokens, SSH authentication keys, Docker configuration files, and more) to/from your EKS pods.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "1dd3b1d5fce4efaeb2e1b5d78ecc2613",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/dns",
    "t": "Managed DNS ",
    "c": "Managed DNS  Best practices  Create an IAM group, add users to the group, and grant the required permissions on the Google Cloud DNS service for the target group. Enable DNSSEC signing on any public-hosted zone to protect against DNS spoofing attacks. Remove unassigned DNS records from your hosted zones (records of resources such as IP addresses that connected to a resource that was removed). Use Google Cloud DNS private zones to manage DNS records for internal resources (such as resources located inside private subnets). Enable Google Cloud DNS audit logs to monitor DNS activity. Note that admin activity audit logs are enabled by default and cannot be disabled. Explicitly enable data access audit logs to log activities in Google Cloud DNS. Limit access to audit logs to the minimum number of employees to avoid unwanted changes to the audit logs.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "c7b9c1ec3e951d6ce55c101eaf4ca650",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/challenges",
    "t": "Growing list of challenges ",
    "c": "Growing list of challenges  Security silos  It is common for many security teams to be left out of DevOps processes and portray security as a separate entity, where specialised people can only maintain and lead security practices. This situation creates a silo around security and prevents engineers from understanding the necessity of security or applying security measures from the beginning. This is not scalable or flexible. Security is a supportive function to help other teams scale and build security, without security teams being a blocker, but rather a ramp to promote secure solutions and decisions. The best practice is to share these responsibilities across all team members instead of having a specialised security engineer. Lack of visibility & prioritisation  Aim to create a culture where security and other essential application components treat security as a regular aspect of the application. Developers can then focus on development with confidence about security instead of security departments playing police and the blame game. Trust can be built between teams, and security can promote the autonomy of teams by establishing processes that instil security. Stringent processes  Every new experiment or piece of software must not go through a complicated process and verification against security compliances before being used by developers. Procedures can be made to be flexible to account for these scenarios, where lower-level tasks are treated differently, and higher-risk tasks and changes are targeted for these more stringent processes. Developers need environments to test new software without common security limitations. These environments are known as “SandBoxes”, which are temporarily isolated environments. These environments have no connection to any internal network and have no customer data. Promote autonomy of teams  Whether it is a large organization or a start-up in hypergrowth, the only way to not leave security behind is by promoting the autonomy of teams. This can be done by automating processes that fit seamlessly with the development pipeline until security tests become just another type of test, like unit testing, smoke bombs, etc. Leading by example and promoting education like creating playbooks/runbooks to spot these flaws and fix them, understand their risk, and build confidence in engineers to make the secure decision independently. The ratio of developers, platform, infrastructure engineers, etc., won’t be the same as security engineers, and we must understand they can’t be in every conversation. Security is a supporting function that focuses on building trust and creating as much overlap in knowledge between teams as possible. Visibility and transparency  For every tool being introduced or practised, there needs to be a supporting process that provides visibility and promotes transparency to other teams. This means that if we want to build autonomy in groups, as mentioned earlier, they need to have visibility on the security state of the service they own or maintain. For example, a dashboard visualises the number of security flaws by the criticality of the service. This helps prioritise accordingly, so tasks don’t get lost in the backlog or noise, and they can tackle flaws at the right time. The security state measure depends on the company, but it could be the number of high findings a service might or might not have which determine if it is in a good security state. Transparency would refer to introducing tools and practices that are accessible to teams. For example, if you present a check before merging code, and the review doesn’t pass and shows a message saying “signature of possible code injection flaw detected, please remediate,” the developer or engineer needs to have access to the tool that is flagging that message. Usually, these analysis tools that flag those alerts have a UI that specifies the line in code where it’s affected. They include a definition and a remediation suggestion with steps. In this example, a developer role can be created so that they have access to more information. This promotes education and autonomy by extending transparency that, traditionally, was only accessible by security teams. Understanding and empathy  There is no magic tool or process for everyone. It is essential to understand how developers/engineers work, what they know to be a risk, and what they prioritise. If you know their perspective, it’s easier to build a process that finds common ground and has a higher chance to work vs adding another tool that creates more noise and stress for everyone. This understanding builds perspective, which accounts for empathy for how other teams work and builds a process that accounts for flexibility. This is needed because every situation might be different, deadlines might be different, and bandwidth can change over time. As a DevSecOps engineer, suppose you took the time to understand how a team owns a service. You can tune the scanners or add a triaging process that tackles the questions they would ask themselves, and this would, in turn, build trust vs crying wolf and security processes being questioned.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "2f14ba3c55b266666a31fb3127d213f6",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/aci",
    "t": "Container Instances (ACI) ",
    "c": "Container Instances (ACI)  ACI is the Azure-managed container orchestration service. It can integrate with other Azure services, such as Azure Container Registry (ACR) for storing containers, Azure AD for managing permissions to ACI, Azure Files for persistent storage, and Azure Monitor. Configuring IAM for ACI  ACI does not have its own authentication mechanism, and it is recommended to use ACR to store container images in a private registry. Authenticate with ACR ACR roles and permissions Encrypt a registry using a customer-managed key Best practices  Grant minimal permissions for accessing and managing ACR, using Azure RBAC. When passing sensitive information (such as credential secrets), make sure the traffic is encrypted in transit through a secure channel (TLS). If you need to store sensitive information (such as credentials), store it inside the Azure Key Vault service. For sensitive environments, encrypt information (such as credentials) using customer-managed keys, stored inside the Azure Key Vault service. Disable the ACR built-in admin user. Conducting auditing and monitoring  Azure allows you to enable logging and auditing using Azure Monitor for containers – a service that allows you to log container-related activities and raise an alarm according to predefined thresholds (for example, low memory resources or high CPU, which requires up-scaling the container environment). Container insights overview Container monitoring solution in Azure Monitor Best practices  Enable audit logging for Azure resources, using Azure Monitor, to log authentication-related activities of your ACR. Limit the access to the Azure Monitor logs to the minimum number of employees to avoid possible deletion or changes to the audit logs. Enabling compliance  Azure security baseline for ACI Azure security baseline for ACR Security considerations for ACI](https://docs.microsoft.com/en-us/azure/container-instances/container-instances-image-security) Introduction to private Docker container registries in Azure Best practices  Use only trusted image containers and store them inside ACR – a private repository for storing your organizational images. Integrate ACR with Azure Security Center, to detect non-compliant images (from the CIS standard). Build your container images from scratch (to avoid malicious code in preconfigured third-party images). Scan your container images for vulnerabilities in libraries and binaries and update your images on a regular basis.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "ce6ae252616a8b6d7cf3ca138132253c",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/functions",
    "t": "Functions ",
    "c": "Functions  Google Cloud Functions is the GCP function as a service. It can integrate with other GCP services, such as Google Cloud IAM for managing permissions to Google Cloud Functions, Google Cloud Audit Logs for monitoring Google Cloud Functions, and Google Cloud Storage for persistent storage. Configuring IAM  Google Cloud IAM is the supported service for managing permissions on Google Cloud Functions. Authorizing access via IAM Authenticating for invocation Securing access with identity Best practices  Use Google Cloud IAM to manage permissions to your Google Cloud functions. Grant minimal permissions for accessing and managing the Google Cloud IAM service. Create a unique service account for each newly created Google Cloud function with minimal permissions using the Google Cloud IAM service. Data and network access  Google Cloud Functions can access resources in your GCP project => it is important to plan before deploying an Google Cloud function. Configuring network settings Connecting to a VPC network Configuring serverless VPC access Using VPC service controls Managing secrets Best practices  Use Google Cloud Functions ingress settings to control which IP address (or CIDR) can access your Google Cloud function. If you need your Google Cloud function to have access to resources inside your VPC, use serverless VPC access to restrict access to your VPC. If your Google Cloud function is connected to your VPC and needs access to external resources, use Google Cloud Functions egress settings to control outbound destinations. If your Google Cloud function needs to have direct inbound access from the internet, use VPC service controls as an extra layer of protection to secure your perimeter from data leakage. For sensitive environments, encrypt Google Cloud Functions environment variables using CMEK management. Use TLS 1.2 to encrypt sensitive data over the network. Conducting auditing and monitoring  Google allows you to enable logging and auditing using the Google Cloud Audit Logs service. Using Cloud audit logging with Cloud Functions Best practices  Admin activity audit logs are enabled by default and cannot be disabled. Explicitly enable data access audit logs to log activities performed on the database. Limit the access to audit logs to the minimum number of employees to avoid possible deletion or changes to the audit logs.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "be8f6326a7b176cd4c0a0a224e5dd3b4",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/db-mysql",
    "t": "Database for MySQL ",
    "c": "Database for MySQL  Configuring IAM  MySQL supports the following types of authentication methods: Local username/password authentication against the MySQL built-in authentication mechanism Azure AD authentication Best practices  For the local MySQL default user, create a strong and complex password, and keep the password in a secured location. For end users who need direct access to the managed database, the preferred method is to use Azure AD authentication . Network access to a managed MySQL  Access to a managed MySQL database service is controlled via firewall rules , which allows you to configure which IP addresses (or CIDR) are allowed to access your managed MySQL database. Best practices  Managed databases must never be accessible from the internet or a publicly accessible subnet – always use private subnets to deploy your databases. Configure the start IP and end IP of your web or application servers, to limit access to the managed database. If you need to manage the MySQL database service, either use an Azure VM (or bastion host) to manage the MySQL database remotely or create a VPN tunnel from your remote machine to the managed MySQL database. Since Azure Database for MySQL is a managed service, it is located outside the customer’s virtual network (VNet). An alternative to secure access from your VNet to Azure Database for MySQL is to use a VNet service endpoint , which avoids sending network traffic outside your VNet, through a secure channel. Stored data  To protect customers’ data, encrypt data both in transport and at rest. Best practices  Security baseline . Enable TLS 1.2 transport layer encryption to your database. For sensitive environments, encrypt data at rest using customer-managed keys stored inside the Azure Key Vault service. Keep your customer-managed keys in a secured location for backup purposes. Enable the soft delete and purge protection features on Azure Key Vault to avoid accidental key deletion (which will harm your ability to access your encrypted data). Enable auditing on all activities related to encryption keys. Conducting auditing and monitoring  As with any other managed service, Azure allows for logging and auditing using built-in services: Built-in Azure Database for MySQL audit logs Azure Monitor logs Best practices  Enable audit logs for MySQL . Configure and access audit logs for Azure Database for MySQL in the Azure portal . Use the Azure Monitor service to detect failed connections . Limit access to the Azure Monitor service data to the minimum number of people to avoid possible deletion or changes to the audit logs. Use Advanced Threat Protection for Azure Database for MySQL to detect anomalies or unusual activity in the MySQL database.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "f86476fe7006d4ab0a9d05251a2a177c",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/ddos",
    "t": "Securing DDoS protection services ",
    "c": "Securing DDoS protection services  Each cloud provider has its own implementation of a managed DDoS protection service. Because cloud providers have very large bandwidth, they can offer (as a paid service) mechanisms to protect customers’ environments from DDoS attacks. These services can help mitigate DDoS attacks: DDoS protection services Auto-scaling groups combined with load-balancing services CDN services WAF services Best practices  AWS Shield Azure DDoS Protection Google Cloud Armor",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "1b03e9a1ed3231323ec2ac48e3969592",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/automation",
    "t": "Security automation ",
    "c": "Security automation  Most development activities include coding, compiling/building, testing, and deployment. In the coding stage, IDE plugins to do security source code analysis can be used. In the build stage, scanning for the secure hardened compiling options and the known vulnerabilities of the dependency components, as well as the secure source code for the whole project. With the build ready and installed on the staging environment, more security scanning can be performed, such as dynamic web security testing by OWASP ZAP, infrastructure configuration security, and secure communication protocols. Production deployment can also be scanned regularly, and this will be more focused on security monitoring instead of the source code or dynamic web security testing. Development  For secure coding, an IDE plugin to do code scanning can be used, or security unit testing and running a static code scan of the whole project. For the secure build delivery, the compiler options must be checked to have been configured properly and all the dependency components must be reviewed for known vulnerabilities. Advantages of using IDE plugins for automated security code review is that the tools can provide recommendations for fixes during coding. This can reduce time spent on code review and fixing security defects that can not be detected by blackbox testing. But this kind of static code scanning may introduce some annoying false positives, and developers may ignore or forget to use the IDE plugins to do static secure code analysis. There may be occasions where peer code review and a team collaboration portal is required. Gerrit provides a web-based UI code review for the GIT source code. Phabricator integrates not code review tools and bug tracking. Developers may choose to not fully apply IDE code-scanning plugins for secure code analysis. In that case, integrating static code analysis into the CI framework can help enforce secure code scanning for all projects. Secure compiler configuration means enabling the compile-time defenses against memory corruption issues to execute unexpected exploit code. These mitigations may include RELRO, address space layout randomization (ASLR), NoExecute (NX), stack canaries, and position-independent executables (PIE). These secure compiler configurations are best done during development. Known vulnerabilities in third-party components or dependencies are part of the OWASP Top 10 List of Using Components with Known Vulnerabilities. Known vulnerable components can be identified at an early development stage. Web testing in proactive/proxy mode  Dynamic web testing tools, such as OWASP ZAP, Arachni, Wapiti and W3af, provide two modes of security testing: proactive and proxy. Proactive mode means the testing tools are launched and test the web services directly. The tester decides the types of security testing (such as XSS or SQLi) of the target web service. With this kind of testing it is possible to miss certain permission-required web pages, or web pages that may require the right order of page visits. Proxy mode, which can also be understood as MITM, means that the security testing tool is running as a proxy and intercepting traffic between the browser client and the target web services. In proxy mode, potential security vulnerability issues are detected based on the intercepted traffic.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "f1c84b20a720f384f5dd09ee65ba6e5f",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/s3",
    "t": "Simple Storage Service (S3) ",
    "c": "Simple Storage Service (S3)  Authentication and authorisation  AWS controls access to S3 buckets using ACLs. Access can be controlled at the entire bucket level (along with all objects inside this bucket) and on a specific object level (for example, let’s suppose you would like to share a specific file with several of your colleagues). The effective permissions of an S3 bucket are the sum of SCPs with identity permissions (IAM policies), total resource permissions (bucket policies), and an AWS KMS policy (such as allowing access to an encrypted object), assuming the user was not denied in any of the previous criteria. Identity and access management in Amazon S3 IAM policies, bucket policies, and ACLs How IAM roles differ from resource-based policies Amazon S3 Preventative Security Best Practices Setting default server-side encryption behaviour for Amazon S3 buckets Consider encryption of data at rest Best practices  Create an IAM group, add users to the IAM group, and grant the required permissions on the target S3 bucket to the target IAM group. Use IAM roles for services (such as applications or non-human identities) that require access to S3 buckets. Restrict access for IAM users/groups to a specific S3 bucket, rather than using wildcard permissions for all S3 buckets in the AWS account. Remove default bucket owner access permissions to S3 buckets. Use IAM policies for applications (or non-human identities)/service-linked roles that need access to S3 buckets. Enable MFA delete for S3 buckets to avoid the accidental deletion of objects from a bucket. Grant minimal permissions to S3 buckets (that is, a specific identity on a specific resource with specific conditions). Use the bucket ACL’s write permissions for the Amazon S3 log delivery group to allow this group the ability to write access logs (for further analysis). For data that you need to retain for long periods (due to regulatory requirements), use the S3 object lock to protect the data from accidental deletion. Encrypt data at rest using Amazon S3-Managed Encryption Keys (SSE-S3). For sensitive environments, encrypt data at rest using Customer-Provided Encryption Keys (SSE-C). Network access  Amazon S3 is a managed service, and located outside the customer’s Virtual Private Cloud (VPC). It is important to protect access to the Amazon S3 service. Internetwork traffic privacy AWS PrivateLink for Amazon S3 Protecting data using encryption Setting default server-side encryption behaviour for Amazon S3 buckets Consider encryption of data at rest Enforce encryption of data in transit Using S3 Object Lock Using presigned URLs Best practices  Unless there is a business requirement to share data publicly (such as static web hosting), keep all Amazon S3 buckets (all tiers) private. To secure access from your VPC to the Amazon S3, use AWS PrivateLink. This keeps traffic internally inside the AWS backbone, through a secure channel, using the interface’s VPC endpoint. For sensitive environments, use bucket policies to enforce access to an S3 bucket from a specific VPC endpoint or a specific VPC. Use bucket policies to enforce the use of transport encryption (HTTPS only). For sensitive environments, use bucket policies to require TLS version 1.2 as the minimum. Encrypt data at rest using SSE-S3. For sensitive environments, encrypt data at rest using SSE-C. Consider using presigned URLs for scenarios where you need to allow external user access (with specific permissions, such as file download) to an S3 bucket for a short period, without the need to create an IAM user. Auditing and monitoring  AWS allows you to enable logging and auditing using Amazon CloudWatch and AWS CloudTrail. Logging Amazon S3 API calls using AWS CloudTrail Logging requests using server access logging Amazon S3 Monitoring and Auditing Best Practices Reviewing bucket access using Access Analyzer for S3 Amazon S3 inventory Best practices  Enable Amazon CloudWatch alarms for excessive S3 usage (for example, a high volume of GET, PUT, or DELETE operations on a specific S3 bucket). Enable AWS CloudTrail for any S3 bucket to log any activity performed on Amazon S3 by any user, role, or AWS service. Limit access to the CloudTrail logs to a minimum number of employees, preferably those with an AWS management account. Enable S3 server access logs to record all access activities as complimentary to AWS CloudTrail API-based logging (for the purpose of future forensics). Use Access Analyzer for S3 to locate S3 buckets with public access or S3 buckets that have access from external AWS accounts. Enable file integrity monitoring to make sure files have not been changed. Enable object versioning to avoid accidental deletion (and to protect against ransomware). Use Amazon S3 inventory to monitor the status of S3 bucket replication (such as encryption on both the source and destination buckets).",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "285183431539480a47d020a299219cae",
    "u": "https://devsecops.tymyrddin.dev/docs/cicd/artifacts",
    "t": "Artifacts ",
    "c": "Artifacts  Supply-chain Levels for Software Artifacts (SLSA)  In its current state, SLSA is a set of incrementally adoptable security guidelines being established by industry consensus. SLSA differs from a list of best practices in its enforceability: it will support the automatic creation of auditable metadata that can be fed into policy engines to give “SLSA certification” to a particular package or build platform. It is designed to be incremental and actionable, and to provide security benefits at every step. SLSA consists of four levels, with SLSA 4 representing the ideal end state. The lower levels represent incremental milestones with corresponding incremental integrity guarantees. Once an artifact qualifies at the highest level, consumers can have confidence that it has not been tampered with and can be securely traced back to source—something that is difficult, if not impossible, to do with most software today. Resources  SLSA version 1.0 Software Supply Chain Best Practices",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "ef8b398cccff770da78a0a5b34d91e25",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/risk-assessment",
    "t": "Risk assessment ",
    "c": "Risk assessment  The first step in the risk assessment process is to assume the software will be attacked and consider the factors that motivate the threat actor. List out the factors such as the data value of the program, the security level of companies who provide resources that the code depends on, the clients purchasing the software, and how big is the software distributed (single, small workgroup or released worldwide). Based on these factors, write down the acceptable level of risk. For instance, a data loss may cause the company to lose millions, especially if they require to pay fines nowadays with GDPR, but eliminating all potential security bugs in the code may cost $40,000. The company and some other stakeholder groups have to decide whether it is worth it; it is also crucial to communicate these tradeoffs. Hence, everyone has an understanding of risk and its implications. From a brand reputation perspective, if the attack causes damage to the company’s image, it costs the company more in the long run than fixing the code. The next step is risk evaluation. Include factors like the worst-case scenario if the attacker has successfully attacked the software. You can demonstrate the worst-case scenario to executives and senior engineers by simulating a ransomware attack. Determine the value of data to be stolen, valuable data such as the user’s identity, credentials to gain control of the endpoints on the network, and data or assets that pose a lower risk. Another factor to consider is the difficulty of mounting a successful attack, in other words, its complexity. For example, if an attacker can gain access to the company’s tool for giving feedback to colleagues or running retrospective meetings, it would have a lower impact than accessing a production environment’s monitoring and alerts system. The high level of risk will not be acceptable, and it is best to mitigate it. For example, a vulnerability can be exploited by anyone running prewritten attack scripts or using botnets to spread the scripts to compromise computers and networks. Users affected are a vital factor. Some attacks only affect one or two users, but the denial of service attack will affect thousands of users when a server is attacked. Moreover, thousands of computers may be infected by the spread of worms. The last factor is the accessibility of the target. Determine whether the target accepts requests across a network or only local access, whether authentication is needed for establishing a connection, or if anyone can send requests. It has more impact if an attacker accesses a production environment vs a sandbox environment used in local playgrounds for people to do labs or tutorials. Types of risk assessments  Qualitative risk assessment  This is the most common type of Risk Assessment that you will find in companies (hopefully). In a Qualitative Risk Assessment, the goal is to assess and classify risk into thresholds like “Low”, “Medium”, and “High”. It systematically examines what can cause harm and what decisions should be made to define or improve adequate control measures. Like all types of Risk Assessments, each level has a priority, where “High” has the most urgency. Even though Qualitative Risk Assessments don’t use numbers, a typical formula to evaluate qualitative risk is: Risk = Severity x Likelihood Severity is the impact of the consequence, and Likelihood is the probability of it happening. It is up to the risk assessor to judge these circumstances. Quantitative risk assessment  The Quantitative Risk Assessment is used to measure risk with numerical values. Instead of Low , Medium , and High , you would have numbers that represent those bands. When carrying out Quantitative Risk Analysis, we can use tools to determine Severity and Likelihood or custom series of calculations based on the company’s processes. For example, suppose there are services with assigned business criticality levels. In that case, you can say that if a bug affects a business-critical service (an authentication service, a payment infrastructure etc.), you will assign 5 points. This highlights why it is vital to understand a security posture and its processes. Measuring risk and priority with an endemic equation to a company’s services will have great results. Real-world  Supply-chain attacks are one of the most serious risks.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "d4cdf9c4e64ad52c8eb2f2210d0ec52e",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/ec2",
    "t": "Elastic Compute Cloud (EC2) ",
    "c": "Elastic Compute Cloud (EC2)  Use only trusted AMI when deploying EC2 instances. Use a minimal number of packages inside an AMI, to lower the attack surface. Use Amazon built-in agents for EC2 instances (backup, patch management, hardening, monitoring, and others). Use the new generation of EC2 instances, based on the AWS Nitro System . Authenticating to an instance  AWS does not have access to customers’ VMs. When running the EC2 launch deployment wizard, you must choose either an existing key pair or create a new key. This set of private/public keys is generated at the client browser – AWS does not have any access to these keys, and therefore cannot log in to your EC2 instance. For Linux instances, the key pair is used for logging in to the machine via the SSH protocol . For Windows instances, the key pair is used to retrieve the built-in administrator’s password Best practices  Keep the private keys in a secured location. An alternative for storing and retrieving SSH keys is to use AWS Secrets Manager . Avoid storing private keys on a bastion host or any instance directly exposed to the internet. An alternative to logging in using SSH, without an SSH key, is to use AWS Systems Manager, through Session Manager . Join Windows or join Linux instances to an Active Directory (AD) domain and use AD credentials to log in to the EC2 instances (and avoid using local credentials or SSH keys completely). Network access to an instance  Access to AWS resources and services such as EC2 instances is controlled via security groups (at the EC2 instance level) or a network access control list (NACL) (at the subnet level), equivalent to the on-premises layer 4 network firewall or access control mechanism. Parameters such as source IP (or CIDR), destination IP (or CIDR), destination port (or predefined protocol), and whether the port is TCP or UDP, can be configured. It is also possible to use another security group as either the source or destination in a security group. Best practices  For remote access protocols (SSH/RDP), limit the source IP (or CIDR) to well-known addresses. Good alternatives for allowing remote access protocols to an EC2 instance are to use a VPN tunnel, a bastion host, or AWS Systems Manager Session Manager . For file sharing protocols (CIFS/SMB/FTP), limit the source IP (or CIDR) to well-known addresses. Set names and descriptions for security groups to allow a better understanding of the security group’s purpose. Use tagging (labelling) for security groups to allow a better understanding of which security group belongs to which AWS resources. Limit the number of ports allowed in a security group to the minimum required ports for allowing your service or application to function. Instance metadata  An example of metadata about a running instance can be retrieved from within an instance, by opening a browser from within the operating system or by using the command line, to a URL such as http://<IP address>/latest/meta-data/ . The IP address is an internal IP and cannot be accessed from outside the instance, but the information, by default, can be retrieved locally without authentication. Instance Metadata Service Version 2 (IMDSv2) allows for enforcing authenticated or session-oriented requests to the instance metadata: aws ec2 modify-instance-metadata-options \\ --instance-id <INSTANCE-ID> \\ --http-endpoint enabled --http-tokens required Serial console connection  For troubleshooting purposes, you can connect using a serial console to resolve network or operating system problems when SSH or RDP connections are not available: aws ec2 enable-serial-console-access --region <Region_Code> This exposes the EC2 instance, so configure access to the EC2 serial console : Best practices  Limit access to the EC2 serial console to the group of individuals using identity and access management (IAM) roles. Only allow access to EC2 serial console when required. Set a user password on an instance before allowing the EC2 serial console. Patch management  To deploy security patches for either Windows or Linux-based instances in a standard manner, use the AWS Systems Manager : Configure the patch baseline. Scan the EC2 instances for deviation from the patch baseline at a scheduled interval. Install missing security patches on your EC2 instances. Review the Patch Manager reports. Best practices  Use AWS Systems Manager Compliance to make sure all your EC2 instances are up to date. Create a group with minimal IAM privileges to allow only relevant team members to conduct patch deployment. Use tagging (labelling) for your EC2 instances to allow patch deployment groups per tag (for example, production versus development environments). For stateless EC2 instances (where no user session data is stored inside an EC2 instance), replace an existing EC2 instance with a new instance, created from an up-to-date operating system image. Backups  The AWS Backup service encrypts backups in transit and at rest using AWS encryption keys, stored in AWS Key Management Service (KMS), as an extra layer of security, independent of Elastic Block Store (EBS) volume or snapshot encryption keys. Best practices  Configure the AWS Backup service with an IAM role to allow access to the encryption keys stored inside AWS KMS. Configure the AWS Backup service with an IAM role to allow access to your backup vault. Use tagging (labelling) for backups to allow a better understanding of which backup belongs to which EC2 instance. Consider replicating backups to another region .",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "9853457369d3e3697c41ee3c448ad7c2",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/client-vpn",
    "t": "Client VPN ",
    "c": "Client VPN  AWS Client VPN allows you to connect to the AWS environment from anywhere on the internet using an OpenVPN client in a secure TLS channel. Best practices  Restrict access to AWS resources inside your AWS environment using VPC security groups and authorisation rules. If you are managing your user identities with AWS Directory Service, use AWS Client VPN Active Directory authentication. If you are using the SAML 2.0 federated authentication service, use AWS Client VPN single sign-on authentication (SAML authentication). For highly sensitive environments, use AWS Client VPN certificate-based authentication using the ACM service. If you are using AWS Client VPN certificate-based authentication, use client certificate revocation lists to revoke access to employees who have left the organization or do not need access through the VPN. Use Amazon CloudWatch to monitor the VPN tunnels (for example, it could send an alarm when the amount of traffic in bytes is above a pre-defined threshold). Use AWS CloudTrail to monitor users’ activity on the AWS VPN service.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "f3ad9fc8def15f36b14e211ec3ea17cc",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/vms",
    "t": "Virtual Machines ",
    "c": "Virtual Machines  Use only trusted images when deploying Azure Virtual Machines. Use a minimal number of packages inside an image, to lower the attack surface. Use Azure built-in agents for Azure Virtual Machines (backup, patch management, hardening, monitoring, and others). For highly sensitive environments, use Azure confidential computing images , to ensure security and isolation of customers’ data. Authenticating to a VM  Microsoft does not have access to customers’ VMs. By running the create a virtual machine wizard you must choose either an existing key pair or create a new key pair. This set of private/public keys is generated at the client side – Azure does not have any access to these keys, and therefore cannot log in to your VM. For Linux instances, the key pair is used for logging in to the machine via the SSH protocol . For Windows machines , you are asked to specify your own administrator account and password to log in to the machine via the RDP protocol. Best practices  Keep credentials in a secured location. Avoid storing private keys on a bastion host (VMs directly exposed to the internet). Join Windows or join Linux instances to an AD domain and use your AD credentials to log in to the VMs (avoiding using local credentials or SSH keys completely). Network access to a VM  Access to Azure resources and services such as VMs is controlled via network security groups , which are equivalent to the on-premises layer 4 network firewall or access control mechanism. Filter network traffic with a network security group using the Azure portal . Best practices  For remote access protocols (SSH/RDP), limit the source IP (or CIDR) to well-known addresses. Good alternatives for allowing remote access protocols to an Azure VM is to use a VPN tunnel, use Azure Bastion , or use Azure Privileged Identity Management (PIM) to allow just-in-time access to a remote VM. For file sharing protocols (CIFS/SMB/FTP), limit the source IP (or CIDR) to well-known addresses. Set names for network security groups to allow a better understanding of the security group’s purpose. Use tagging (labelling) for network security groups to allow a better understanding of which network security group belongs to which Azure resources. Limit the number of ports allowed in a network security group to the minimum required ports for allowing a service or application to function. Serial console connection  For troubleshooting purposes, you can connect using a serial console ( Azure Serial Console for Linux and Azure serial console for Windows ) to resolve network or operating system problems when SSH or RDP connections are not available. The following commands use the Azure CLI tool to allow serial access for the entire Azure subscription level: subscriptionId=$(az account show --output=json | jq -r .id) az resource invoke-action --action enableConsole \\ --ids \"/subscriptions/$subscriptionId/providers/Microsoft.SerialConsole/consoleServices/default\" --api-version=\"2018-05-01\" This exposes the VM instance. Best practices  Access to the serial console should be limited to the group of individuals with the Virtual Machine Contributor role for the VM and the Boot diagnostics storage account. Always set a user password on the target VM before allowing access to the serial console. Patch management  To deploy security patches for either Windows or Linux-based instances in a standard manner, it is recommended to use Azure Automation Update Management : Create an automation account. Enable Update Management for all Windows and Linux machines . Configure the schedule settings and reboot options. Install missing security patches on your VMs. Review the deployment status. Best practices  Use minimal privileges for the account using Update Management to deploy security patches. Use update classifications to define which security patches to deploy. When using an Azure Automation account, encrypt sensitive data (such as variable assets). When using an Azure Automation account, use private endpoints to disable public network access. Use tagging (labelling) for your VMs to allow defining dynamic groups of VMs (for example, prod versus dev environments). For stateless VMs (where no user session data is stored inside an Azure VM), replace an existing Azure VM with a new instance, created from an up-to-date operating system image. Backups  The Azure Backup service encrypts backups in transit and at rest using Azure Key Vault. Best practices  Use Azure role-based access control (RBAC) to configure Azure Backup to have minimal access to your backups. For sensitive environments, encrypt data at rest using customer-managed keys. Use private endpoints to secure access between your data and the recovery service vault. If you need your backups to be compliant with a regulatory standard, use Regulatory Compliance in Azure Policy . Use Azure security baselines for Azure Backup (Azure Security Benchmark). Enable the soft delete feature to protect your backups from accidental deletion. Consider replicating your backups to another region .",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "0949283c6177a80676655698b5d31fea",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/dns",
    "t": "Securing DNS services ",
    "c": "Securing DNS services  Each cloud provider has its own implementation of managed DNS services – these include services for translating hostnames into IP addresses, different types of DNS records services, resolving hostname to load-balance IP, etc. Best practices  Amazon Route 53 Azure Managed DNS Google Cloud Managed DNS",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "7928236196cb6ab060526ddbbecf3512",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/s2s-vpn",
    "t": "Site-to-Site VPN ",
    "c": "Site-to-Site VPN  AWS Site-to-Site VPN is a managed service connecting corporate networks to the AWS environment in a secure IPsec channel. Best practices  Restrict access to AWS resources inside your AWS environment using Amazon VPC security groups and authorisation rules. For non-sensitive environments, use pre-shared keys to authenticate to the site-to-site VPN tunnel. For highly sensitive environments, use a private certificate from the AWS Certificate Manager (ACM) Private Certificate Authority (CA) service. Create an IAM group, add users to the group, and grant the required permissions on the AWS Site-to-Site VPN connection for the target group an example of an IAM role would be the ability to invoke an API action through the VPN). It is recommended to schedule a maintenance window and rotate the pre-shared keys or the certificate for the AWS Site-to-Site VPN connection once a year, to avoid potential compromise. Use Amazon CloudWatch to monitor the VPN tunnels (for example, it could send an alarm when the amount of traffic in bytes is above a pre-defined threshold). Use AWS CloudTrail to monitor users’ activity on the AWS VPN service.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "3ac2cafc992646c241b6cea234655299",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/iam",
    "t": "Identity management ",
    "c": "Identity management  Identity and Access Management (IAM) refers to the concept of managing the entire user life cycle, including provisioning (that is, account creation), assigning permissions, and, finally, account deprovisioning (that is, when a person leaves the organisation, or the account is no longer needed). Access management is made up of the following main concepts: Identity indicates a user, computer, service, or role that wishes to access a system or application and take actions (from accessing shared storage to querying a database and pulling information). Authentication is the process of proving that a specific identity (such as a user) is who they claim to be (for example, providing a username and password that match the user’s records on a central user repository). Authorisation is the process of granting an authenticated identity (such as a user) the permissions to take actions on a resource (such as allowing a user to upload a file to shared storage). AD  Understanding AD is not easy, but basic knowledge is necessary when talking about IAM. An organisation should only have one central directory. Identities should only be kept in one place. That also comes with a risk: if a directory gets breached, an attacker will have access to all identities that exist within the organisation. It’s crucial that the directory and the IAM system are very secure and that directory data is extremely well protected. This is an area where tools such as Saviynt and CyberArk come in; they add an extra security layer on top of IAM. The term AD is very much associated with Microsoft, as it was developed by that company for Windows domain networks. It has become a widely accepted term for the concept itself. AD comprises basically two major components that are both relevant in cloud environments. The first component is the directory itself; the second component is the domain services. Domain services comprise a domain controller that authorizes and authenticates objects (users and computers) in a network. That network can be in a public cloud. It can also be a standalone network, but more often, the internal network of the organisation is extended to a cloud. Extended may not be the right word, though. The organisation’s on-premises network and cloud network(s) are merely connected or, to put it a better way, the domains are connected. To be able to do that, domain controllers are needed in the public cloud. The domain controller makes sure that a specific part of the public cloud is now within the domain. Microsoft AD uses LDAP, Kerberos, and Domain Name Services for these services. LDAP enables the authentication and storing of data about objects and also applications. Kerberos is used to prove the identity of an object to another object that it communicates with. DNS enables the translation of IP addresses to domain names and vice versa. AAD  Azure uses Azure Active Directory (AAD). AAD is not the same as AD. AAD is an authentication service in Azure, using AD as the directory. The primary function of AAD is to synchronize identities to the cloud. For the synchronisation, it uses Azure AD Connect. With AAD, organisations will have a system that provides people of these organisations with a mechanism to log in and access resources on different platforms. That can be resources in Azure itself or resources such as applications hosted on systems in the corporate network. AAD also provides access to SaaS solutions such as Microsoft 365 and applications that can integrate with Azure. AAD makes sure that users only have to log in once using SSO. It is secured by MFA, meaning that when a user logs in by typing in a password, it is not enough. A second validation is needed to prove their identity. This can be a PIN code through a text message or an authenticator app on a mobile device, but also a fingerprint. If the user is authenticated, access is granted to federated services. The federation between the domains in the corporate network and the corporate domain in Azure cloud is done with Active Directory Federation Services (ADFS). Strictly speaking, there’s no hard requirement to use ADFS since AAD integrates with AD natively with hybrid entities, using password hash-sync or passthrough authentication. For third-party MFA you will still need ADFS though. In the cloud, a corporate cloud domain is situated in the domain of the public cloud itself. In Azure, that is defined by onmicrosoft.com; this domain name address signifies that an environment resides in Azure. In AWS, ADFS can be enabled as a component of the AWS Federated Authentication service. With ADFS, a user is authenticated against the central identity store, AD. After authentication, ADFS returns an assertion that permits login to AWS using AWS’s Security Token Service (STS). STS returns temporary credentials based on AssumeRoleWithSAML , which then allows access to the AWS Management Console of the enterprise environments in AWS. AssumeRoleWithSAML is something specific to AWS. This function in STS provides a way to authenticate against the identity store with role-based AWS access. It uses Security Assertion Markup Language (SAML), an open standard for exchanging authentication and authorisation data between parties, such as the identity store at a corporate level and the cloud provider. It is comparable to LDAP, but SAML is more commonly used in the cloud. Also, GCP embraces SAML to do AD federation. At GCP, it starts with Google Cloud Identity, the service that GCP uses for IAM. But Google also understands that enterprises typically already have an identity store with AD. We can set up federation between GCP’s Cloud Identity or G Suite and enable user provisioning from AD, including SSO. SSO is done through SAML. A lot of companies still use ADFS, but it is no longer a hard requirement to use AD in AWS or GCP. It is possible to integrate AD directly into other clouds. Cloud providers  Cloud providers take a different approach to IAM: AWS IAM : By default, all requests are implicitly denied. Azure AD : By default, users have a minimal set of permissions in which to access resources. Google Cloud IAM : By default, service accounts have permission to call Google Cloud APIs.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "2b852c36be6321d68033da3138be395c",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/coding",
    "t": "Secure coding ",
    "c": "Secure coding  Secure coding rules and best practices are guidelines. They require the right secure coding tools to make them happen, and also the right approaches to make them more effective and efficient. Secure coding awareness training  Case studies or scenario-based vulnerable source code examples will have better training effects than simply secure coding rules. OWASP Security Knowledge Framework Android Application Secure Design and Secure Coding Guidebook Find Security Bugs Patterns for Java OWASP Teammentor Tool evaluation  When the importance and the challenge of secure coding becomes apparent, people will look for some tools to make the secure coding easier. Some evaluation considerations that have proven useful to others: Considerations Description Usability The target users of the code scanning tools are developers. The usability includes the capability to scan parts of the source code, differential scans, scanning reports, tracing back to original source code, and so on. Budget If it's an IDE plugin commercial tool, we need to consider how many concurrent users' licenses it will need. Programming languages support Most tools support C/C++ and Java, but do not support script languages, such as Python, JavaScript, or PHP. Do a survey of the programming languages used by in-house projects and prioritise the programming languages that are going to be supported. Detection rate and false positive rates It is common for any scanning tools to have false positive rates, depending on the scanning engine and rules. A high false positive is not a bad thing, and it can also mean the scanner takes a more conservative approach. Find the tool that best fits the projects instead of the most well-known. To evaluate the detection rate, use known vulnerable projects. Scanning rules update It is important that the tool is constantly updated with rules and scanners. After using the code scanning tools for a while, a security team may help to optimise the tools, processes, or rules based on user feedback. Secure compiling  Memory corruption and buffer overflow may result in exploit code injection attacks . For the C/C++ programming language, these can be protected by compiler options: SAFECode Development Practices OWASP C-based ToolChain Hardening Linux Audit ASLR MS Security Best Practice for C++ Secure Compiler and linker flags for GCC To verify whether the application or the environment has been configured with secure options, these can be useful: CheckSec BinScope",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "fe3bdfdb3ceee982d491db7fe68ff573",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/readme",
    "t": "Introduction ",
    "c": "Introduction  What?  Securing Azure services. Why?  To be able to support the desperate better. How?  Virtual Machines Database for MySQL Container Instances (ACI) Kubernetes Service (AKS) Functions Blob storage Managed disks Files Container Storage Interface (CSI) Virtual Network (VNet) Managed DNS Content delivery network (CDN) Site-to-Site VPN Point-to-Site VPN DDoS Protection WAF Azure AD",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "24f7d16d3524faeced254a2dc23cb5e3",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/lambda",
    "t": "AWS Lambda ",
    "c": "AWS Lambda  AWS Lambda is the Amazon serverless service. It can integrate with other AWS services, such as AWS IAM for managing permissions to AWS Lambda, Amazon CloudWatch for monitoring AWS Lambda, and Amazon S3 and Amazon EFS for persistent storage. Configuring IAM  AWS IAM is the supported service for managing permissions to AWS Lambda. Identity-based IAM policies for Lambda Security best practices Encrypting Lambda environment variables serverless-puresec-cli Best practices  Grant minimal IAM permissions for any newly created AWS Lambda function (for running tasks, accessing S3 buckets, monitoring using CloudWatch Events, and so on) – match a specific IAM role to any newly created AWS Lambda function. Use open source tools such as serverless-puresec-cli to generate IAM roles for your function. Avoid storing credentials inside AWS Lambda code. If you need to store sensitive data (such as credentials), use AWS Secrets Manager. For better protection of your Lamba functions, configure AWS Lambda behind Amazon API Gateway. For sensitive environments, encrypt Lambda environment variables using CMK management (as explained in Chapter 7, Applying Encryption in Cloud Services). Use TLS 1.2 to encrypt sensitive data over the network. Enforce MFA for end users who have access to the AWS API (console, CLI, and SDK) and perform privileged actions such as managing the Lambda service. Network access to AWS Lambda  AWS Lambda can be deployed either as an external resource outside a VPC or inside a VPC => it is important to plan before deploying each Lambda function. Data protection in AWS Lambda AWS Lambda now supports AWS PrivateLink Configuring a Lambda function to access resources in a VPC How do I give internet access to a Lambda function that’s connected to an Amazon VPC? Best practices  Use Amazon API Gateway to restrict access to your Lambda function, from a specific IP address or CIDR. If your Lambda function is located outside a VPC, and the Lambda function needs access to resources inside your VPC, use AWS PrivateLink, which avoids sending network traffic outside your VPC, through a secure channel, using an interface VPC endpoint. If your Lambda function is located inside your VPC, and the Lambda function needs access to external resources on the internet, use the NAT gateway to give your Lambda function the required access, without exposing Lambda to the internet directly. Use TLS 1.2 to encrypt traffic to and from your Lambda functions. Conducting auditing and monitoring  AWS allows you to enable auditing using the AWS CloudTrail service. Using AWS Lambda with AWS CloudTrail Using AWS Lambda with Amazon CloudWatch Events Best practices  Enable enhanced monitoring of your Lambda functions. Use Amazon CloudWatch to detect spikes in Lambda usage. Use AWS CloudTrail to monitor API activities related to your Lambda function. Conducting compliance, configuration change, and secure coding  As a customer, you cannot control the underlying infrastructure => invest in secure coding to avoid attackers breaking into your application and causing harm that AWS cannot protect. Using AWS Lambda with AWS Config Lambda function versions Use API Gateway Lambda authorisers Setting up automatic assessment runs through a Lambda function Configuring code signing for AWS Lambda OWASP Serverless Top 10 Best practices  Follow the OWASP Serverless Top 10 project documentation when writing your Lambda function code. Enable versions in your Lambda functions, to be able to roll back to previous code. Use AWS Signer to sign your Lambda function code and make sure you only run signed code. If you use Amazon API Gateway in front of your Lambda functions, use the API Gateway Lambda authoriser as an extra layer of protection for authorizing access to your Lambda functions. Use AWS Config to check for changes in your Lambda functions. Use Amazon Inspector assessment templates to detect non-compliance or the use of old versions of a runtime in your Lambda functions.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "a62c146e9300ca92b78ce6f8dad9b81a",
    "u": "https://devsecops.tymyrddin.dev/docs/cicd/docker",
    "t": "Docker ",
    "c": "Docker  Configuration best practices  Patch both Docker Engine and the underlying host operating system running Docker. The kernel is shared by the container and the host. A successful kernel exploit can enable attackers to break out of a non-privileged container and gain root access to the host. The Docker daemon socket is a Unix network socket that facilitates communication with the Docker API. By default, this socket is owned by the root user. If anyone else obtains access to the socket, they will have permissions equivalent to root access to the host. Daemon sockets can be bound to a network interface for making the Docker container available remotely. Never make the daemon socket available for remote connections, unless you are using Docker’s encrypted HTTPS socket, which supports authentication. Do not run Docker images with an option like -v /var/run/docker.sock://var/run/docker.sock , which exposes the socket in the resulting container. Docker provides rootless mode , which allows for running Docker daemons and containers as non-root users, to mitigate vulnerabilities in daemons and container runtimes, which can grant root access of entire nodes and clusters to an attacker. Rootless mode runs Docker daemons and containers within a user namespace. Install Docker in root mode and launch the Daemon when the host starts with: systemctl --user enable docker sudo loginctl enable-linger $(whoami) To run a container as rootless using Docker context: docker context use rootless docker run -d -p 8080:80 nginx Docker provides a privileged mode, which lets a container run as root on the local machine. Running a container in privileged mode provides the capabilities of that host. This includes Root access to all devices; The ability to tamper with Linux security modules like AppArmor and SELinux; And the ability to install a new instance of the Docker platform, using the host’s kernel capabilities, and running Docker within Docker. Never use privileged containers in a production environment. Better yet, never use in any environment. To check if the container is running in privileged mode (returns true if the container is privileged): docker inspect --format =''[container_id] The default setting is to allow the container to access all RAM and CPU resources on the host. When a container is compromised, attackers may try to make use of the underlying host resources. Limit Docker memory and CPU usage to minimise the impact of breaches for resource-intensive containers. Docker containers require a network layer to communicate with the outside world through the network interfaces on the host. The default bridge network exists on all Docker hosts. New containers automatically connect to it. Recommended is using custom bridge networks to control which containers can communicate between them, and to enable automatic DNS resolution from container name to IP address. Create as many networks as needed, decide which networks each container needs to connect to, and avoid connecting sensitive containers to public-facing networks. Docker provides network drivers for creating user-defined bridge networks , overlay networks , or macvlan networks . Create an optimised environment to run containers. Operating systems on a container host are to protect the host kernel from container escapes and prevent mutual influence between containers. Protecting a container is exactly the same as protecting any process running on Linux. Use one or more of the following Linux security capabilities: Linux Namespaces make Linux processes appear to have access to their own, separate global resources. Namespaces provide an abstraction that gives the impression of running in a container on its own operating system. They are the basis of container isolation. For Red Hat Linux distributions, SELinux provides an additional layer of security to isolate containers from each other and from the host. It allows for applying mandatory access controls for users, applications, processes and files. A second line of defense that will stop attackers who manage to breach the namespace abstraction. For Debian Linux distributions, AppArmor is a Linux kernel enhancements that can limit programs in terms of the system resources they can access. It binds access control attributes to specific programs, and is controlled by security profiles loaded into the kernel at boot time. Use cgroups to prevent container resources from being used by other containers on the same host, and at the same time, stop attackers from creating pseudo devices. Linux allows for limiting privileges of any process, containers included. When running a container, you can deny privileges for capabilities, without affecting containerised applications. The secure computing mode ( seccomp ) in the Linux kernel allows for transitioning a process to a secure mode, in which it is only allowed to perform a small set of safe system calls. Setting a seccomp profile for a container provides one more level of defense against compromise. A simple and effective security trick is to run containers with a read-only filesystem. This can prevent malicious activity such as deploying malware on the container or modifying configuration files. Cloud native security requires security controls and mitigation techniques at every stage of the application lifecycle, from build to workload and infrastructure. Implement vulnerability scanning to ensure clean code at all stages of the development lifecycle. Use a sandbox environment to QA code before it goes into production, to ensure there is nothing malicious that will deploy at runtime. Implement drift prevention to ensure container immutability. Create an incident response process to ensure rapid response in the case of an attack. Apply automated patching. Ensure robust auditing and forensics for quick troubleshooting and compliance reporting. Not all system calls are required to run a container. Monitor the container, obtain a list of all system calls made, and explicitly allow those calls and no others. Using runtime for this you can also take system calls used by the container’s components into account, and how those calls are named in the underlying operating system. Image best practices  Scan Docker container images before use, especially if they were pulled from public repositories. Any vulnerability in any component of an image will exist in all containers created from it. If you use a base image to create new images, any vulnerability in the base image will extend to the new images. When building an image from the CI pipeline, scan it before running it through the build. Images with vulnerabilities that exceed a severity threshold are to fail the build. Do not push unsafe images to a container registry accessible by production systems. Docker scout can proactively find and fix vulnerabilities. Docker has integrated with Snyk to help scan official images, which makes it easier to scan images locally and immediately after build. Docker images are commonly built on top of “base images”. A base image may contain components not really required for your purposes. Select base images to fit purpose, and if necessary, build your own minimal base image. Docker images often require sensitive data for their normal operations, such as credentials, tokens, SSH keys, TLS certificates, database names or connection strings. And applications running in a container can generate or store sensitive data. Do not hardcode sensitive information into Dockerfiles. Such information will be copied to containers, and may be cached in intermediate container layers, even if deleted. Container orchestrators like Kubernetes and Docker Swarm provide a secrets management capability which can solve this problem. To build containerised applications in a consistent manner, use multi-stage builds. This has both operational and security advantages. A well-designed multi-stage build contains only the minimal binary files and dependencies...",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "4558b97f5061378daf40df3a77d571a7",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/waf",
    "t": "WAF ",
    "c": "WAF  Azure WAF is integrated with and can be deployed as part of: Azure Application Gateway: A Layer 7 load balancer service Azure Front Door: A global web application threat protection service Azure CDN: A managed CDN Best practices  Deploy Azure WAF v2 license on any newly exposed web application. For large-scale environments with multiple Azure subscriptions and multiple web applications, use Azure WAF with Azure Front Door to protect your web applications. After learning the traffic for your production web application (running WAF in learning mode), configure Azure WAF in prevention mode. For protection against non-standard types of web application attacks, create your own custom rules. Create custom rules to block traffic originating from known malicious IP addresses. Use Azure activity logs as part of the Azure Monitor service to monitor changes in Azure WAF rules. Send Azure WAF logs to Azure Sentinel to detect and remediate web-based attacks. Use Azure Active Directory to limit the permissions to the Azure WAF console.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "646c503f20b11370e982e080c212070f",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/cdn",
    "t": "Content delivery network (CDN) ",
    "c": "Content delivery network (CDN)  Best practices  Restrict access to origin servers (where your original content is stored) from CDN segments only (allow traffic only from CDN segments towards servers or services that store content). Share content via HTTPS protocol to preserve the confidentiality of the content and to assure the authenticity of the content. When distributing content over HTTPS, use TLS 1.2 over older protocols such as SSL v3. If you have a requirement to distribute content such as individual files for a short period of time, use signed URLs. Enable Google Cloud CDN audit logs to monitor CDN activity. Note that admin activity audit logs are enabled by default and cannot be disabled. Explicitly enable data access audit logs to log activities in Google Cloud CDN. Limit access to audit logs to the minimum number of employees to avoid unwanted changes to the audit logs.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "40192449a6ecc55634322cc08eda40c6",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/eks",
    "t": "Elastic Kubernetes Service (EKS) ",
    "c": "Elastic Kubernetes Service (EKS)  EKS is the Amazon-managed Kubernetes orchestration service. It can integrate with other AWS services, such as Amazon ECR for storing containers, AWS IAM for managing permissions to EKS, and Amazon CloudWatch for monitoring EKS. Configuring IAM  AWS IAM is the supported service for managing permissions to access and run containers through Amazon EKS. How Amazon EKS works with IAM IAM roles for service accounts IAM Controlling Access to EKS Clusters Best practices  Grant minimal IAM permissions for accessing and managing Amazon EKS. If you are managing multiple AWS accounts, use temporary credentials (using AWS Security Token Service or the AssumeRole capability) to manage EKS on the target AWS account with credentials from a source AWS account. Use service roles to allow the EKS service to assume your role and access resources such as S3 buckets and RDS databases. For authentication purposes, avoid using service account tokens. Create an IAM role for each newly created EKS cluster. Create a service account for each newly created application. Always run applications using a non-root user. Use IAM roles to control access to storage services (such as Amazon EBS, Amazon EFS, and Amazon FSx for Lustre) from EKS. Enforce MFA for end users who have access to the AWS console and perform privileged actions such as managing the EKS service. Store your container images inside Amazon ECR and grant minimal IAM permissions for accessing and managing Amazon ECR. Network access  Amazon EKS is a managed service, and located outside the customer’s VPC. An alternative to secure access from a VPC to the managed EKS environment is to use AWS PrivateLink, which avoids sending network traffic outside your VPC, through a secure channel, using an interface VPC endpoint. Network security Amazon EKS networking Introducing security groups for Pods EKS best practice guides Best practices  Use TLS 1.2 to control Amazon EKS using API calls. Use TLS 1.2 when configuring Amazon EKS behind AWS Application Load Balancer or AWS Network Load Balancer. Use TLS 1.2 between your EKS control plane and the EKS cluster’s worker nodes. Use VPC security groups to allow access from your VPC to the Amazon EKS VPC endpoint. Use VPC security groups between your EKS control plane and the EKS cluster’s worker nodes. Use VPC security groups to protect access to your EKS Pods. Disable public access to your EKS API server – either use an EC2 instance (or bastion host) to manage the EKS cluster remotely or create a VPN tunnel from your remote machine to your EKS cluster. If you use AWS Secrets Manager to store sensitive data (such as credentials) from Amazon EKS, use a Secrets Manager VPC endpoint when configuring security groups. Store your container images inside Amazon ECR, and for non-sensitive environments, encrypt your container images inside Amazon ECR using AWS KMS. For sensitive environments, encrypt your container images inside Amazon ECR using CMK management. If you use Amazon ECR to store your container images, use VPC security groups to allow access from your VPC to the Amazon ECR interface VPC endpoint. Conducting auditing and monitoring  AWS allows for logging and auditing using CloudWatch and AWS CloudTrail. Amazon EKS control plane logging Auditing and logging Best practices  Enable the Amazon EKS control plane when logging in to Amazon CloudWatch – this allows you to log API calls, audit, and authentication information from the EKS cluster. Enable AWS CloudTrail for any action performed on the EKS cluster. Limit the access to the CloudTrail logs to the minimum number of employees – preferably in an AWS management account, outside the scope of your end users (including outside the scope of your EKS cluster administrators), to avoid possible deletion or changes to the audit logs. Enabling compliance  Configuration and vulnerability analysis in Amazon EKS Introducing the CIS Amazon EKS Benchmark Compliance Image security Pod security kube-bench Docker Bench for Security Amazon ECR private repositories Best practices  Use only trusted image containers and store them inside Amazon ECR – a private repository for storing your organizational images. Run the kube-bench tool on a regular basis to check for compliance with CIS Benchmarks for Kubernetes. Run the Docker Bench for Security tool on a regular basis to check for compliance with CIS Benchmarks for Docker containers. Build your container images from scratch (to avoid malicious code in preconfigured third-party images). Scan your container images for vulnerabilities in libraries and binaries and update your images on a regular basis. Configure your images with a read-only root filesystem to avoid unintended upload of malicious code into your images.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "fe089660647312d36c14fde44cadfd81",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/blob",
    "t": "Blob storage ",
    "c": "Blob storage  Authentication and authorisation  Azure controls authorisation for Blob storage using Azure Active Directory. For temporary access to Azure Blob storage (that is, for an application or a non-human interaction), use shared access signatures (SAS). Authorise access to blobs using Azure Active Directory Prevent Shared Key authorisation for an Azure Storage account Grant limited access to Azure Storage resources using shared access signatures (SAS) Security recommendations for Blob storage Best practices  Create an Azure AD group, add users to the AD group, and then grant the required permissions on the target Blob storage to the target AD group. Use shared key authorisation (SAS) to allow applications temporary access to Blob storage. Grant minimal permissions to Azure Blob storage. For data that you need to retain for long periods (due to regulatory requirements), use an immutable Blob storage lock to protect the data from accidental deletion. Network access  Because Azure Blob storage is a managed service, and located outside the customer’s Virtual Network (VNet). It is important to protect access to the Azure Blob storage service. Configure Azure Storage firewalls and virtual networks Tutorial: Connect to a storage account using an Azure Private Endpoint Require secure transfer to ensure secure connections Enforce a minimum required version of Transport Layer Security (TLS) for requests to a storage account Azure Storage encryption for data at rest Customer-managed keys for Azure Storage encryption Best practices  Keep all Azure Blob storage (that is, all tiers) private. To secure access from your VNet to the Azure Blob storage, use an Azure private endpoint, which avoids sending network traffic outside your VNet through a secure channel. Unforce the use of transport encryption (HTTPS only) for all Azure Blob storage. For sensitive environments, require a minimum of TLS version 1.2 for Azure Blob storage. Deny default network access to the Azure storage account and only allow access from predefined conditions such as the setting up of IP addresses. Encrypt data at rest using Azure Key Vault. For sensitive environments (for example, which contain PII, credit card details, healthcare data, and more), encrypt data at rest using customer-managed keys (CMKs) stored inside Azure Key Vault. Auditing and monitoring  Azure allows you to monitor blob storage using Azure Monitor and Azure Security Center. Monitoring Azure Blob storage Azure Storage analytics logging Log alerts in Azure Monitor Best practices  Enable log alerts using the Azure Monitor service to track access to the Azure Blob storage and raise alerts (such as multiple failed access attempts to Blob storage in a short period of time). Enable Azure storage logging to audit all authorisation events for access to the Azure Blob storage. Log anonymous successful access attempts to locate an unauthorised access attempt to the Azure Blob storage. Enable Azure Defender for Storage to receive security alerts in the Azure Security Center console.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "6530ba40a329de8e19c4cb6072c2ed25",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/containers",
    "t": "Securing containers ",
    "c": "Securing containers  Containers have the following benefits over VMs : Small footprint: Only required libraries and binaries are stored inside a container. Portability: An application can be developed inside a container on a laptop and run at a large scale in a production environment with hundreds or thousands of container instances. Fast deployment and updates compared to VMs. Moving to production and running hundreds of container instances, an orchestrator (mechanism or managed service) is needed for managing container deployment, health check monitoring, container recycling, etc. Docker was adopted by the industry as a de facto standard for wrapping containers, and cloud vendors support a new initiative for wrapping containers called the Open Container Initiative (OCI) . AWS offers OCI artifact support in Amazon ECR , Azure OCI images , and Google Cloud an OCI image format . Kubernetes is an open source project (developed initially by Google) and is now considered the industry de facto standard for orchestrating, deploying, scaling, and managing containers. Best practices  Amazon Elastic Container Service (ECS) Amazon Elastic Kubernetes Service (EKS) Azure Container Instances (ACI) Azure Kubernetes Service (AKS) Google Kubernetes Engine (GKE)",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "94080fd173292b698c52e8fd8a7e46f0",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/gke",
    "t": "Kubernetes Engine (GKE) ",
    "c": "Kubernetes Engine (GKE)  GKE is the Google-managed Kubernetes orchestration service. It can integrate with other GCP services, such as Google Container Registry for storing containers, Google Cloud IAM for managing permissions to GKE, Google Filestore for persistent storage, and Google Cloud operations for monitoring. Configuring IAM  Google Cloud IAM is the supported service for managing permissions to access and run containers through GKE. Creating IAM policies Configuring RBAC Enforce least privilege with recommendations Use least privilege Google service accounts Secret management Best practices  Grant minimal permissions for accessing and managing the Google Cloud IAM service, using Kubernetes RBAC. Use Google Groups to manage permissions to your GKE cluster. Use the Google IAM recommender to set the minimal permissions for your GKE cluster. Create a unique service account with minimal permissions for any newly created GKE cluster. Enforce the use of MFA for any user who needs access to manage your GKE cluster. If you need to store sensitive information (such as credentials), store it inside the Google Cloud KMS service. For sensitive environments, encrypt information (such as credentials) using CMEKs stored inside the Google Cloud KMS service. Network access  GKE exposes services to the internet => it is important to plan before deploying a GKE cluster. Creating a private cluster Restrict network access to the control plane and nodes Restrict traffic among Pods with a network policy Network security Harden workload isolation with a GKE sandbox Best practices  Create private GKE clusters to avoid exposing the GKE cluster control plane (API server) to the public internet – use alias IP ranges to configure which IPs can access the GKE cluster. Use authorised networks to configure who can access your GKE cluster control plane. Use VPC-native networking to protect the access between the Kubernetes Pods. Use network policies for Kubernetes to protect the access between the Kubernetes Pods. Use shielded GKE nodes as an additional layer of protection to your GKE cluster nodes. Create separate namespaces for your applications, according to RBAC requirements. Enable a GKE sandbox to achieve better isolation of your GKE cluster Pods. Use TLS 1.2 to control your GKE cluster using API calls. Use TLS 1.2 between your GKE control plane and the GKE cluster’s nodes. Use TLS 1.2 when configuring the GKE cluster behind Google Load Balancer. Disable public access to your GKE cluster API server – use a Google VM (or a Bastion host) to manage the GKE cluster remotely. Conducting auditing and monitoring  Google allows you to enable logging and auditing using the Google Cloud Logging service – a service that allows you to audit container-related activities. Audit policy Overview of Google Cloud’s operations suite for GKE Remediating security health analytics findings Best practices  Enable logging for any newly created GKE cluster and integrate the item with the Google Cloud Logging service, to log all audit activities related to your GKE cluster. When using a container-optimised operating system image, make sure you send its Linux audit logs to the Google Cloud Logging service. Limit the access to the Google Cloud Logging service logs to the minimum number of employees to avoid possible deletion or changes to the audit logs. Enabling compliance  Container threat detection conceptual overview GKE CIS 1.1.0 Benchmark Inspec Profile GKE auditor Node images Binary authorisation Container Registry Best practices  Use only trusted image containers and store them inside Google Container Registry – a private repository for storing your organizational images. Always use the latest build of Kubernetes on both your GKE cluster and cluster nodes. Use the GKE auditor to detect GKE misconfigurations. Use container-optimised operating systems when creating new container images, for better security. Build your container images from scratch (to avoid malicious code in preconfigured third-party images). Scan your container images for vulnerabilities in libraries and binaries and update your images on a regular basis. Use Google Binary authorisation to make sure you use only signed container images from trusted authorities. Use container threat detection to detect attacks against your container images in real time.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "6065f9eb1afce18c929a32208003f54b",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/shared",
    "t": "Shared responsibility ",
    "c": "Shared responsibility  The shared responsibility model tries to draw a line between the cloud provider and the customer’s responsibilities regarding security. The cloud provider is always responsible for the lower layers – from the physical security of their data centers, through networking, storage, host servers, and the virtualisation layers. Above the virtualisation layer is where the responsibility begins to change, depending on service and cloud provider. AWS  Azure  GCP ",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "b4e71ada6439f81e747e2be953342ecb",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/ddos",
    "t": "DDoS Protection ",
    "c": "DDoS Protection  Azure DDoS Protection is the Azure managed DDoS protection service. It comes in two price models: Azure DDoS Protection Basic: This provides Layers 3/4 (Network layer) and Layer 7 DDoS protection (HTTP/HTTPS), offered at no cost. Azure DDoS Protection Standard: This provides Layers 3/4 (Network layer) and Layer 7 DDoS protection (HTTP/HTTPS), with additional protection at the VNet level, with extra logging and alerting capability. Best practices  Enable Azure DDoS Protection Basic for any production environment you expose to the internet. Use Azure DDoS Protection Standard for large-scale production environments you expose to the internet for better insights into attacks. When using Azure DDoS Protection Standard, enable resource logs for public IP addresses to have quicker detection of attacks. When combining Azure Application Gateway with a WAF, you add protection against web application attacks. Use Azure Monitor to monitor and alert you when there is a spike in incoming requests to have a preliminary alert on incoming DoS attacks. Send Azure DDoS Protection logs to Azure Sentinel for further analysis of DDoS attacks. Use Azure Active Directory to limit the permissions to the Azure DDoS Protection Console. When using Azure DDoS Protection Standard, you can conduct simulations of DDoS attacks against your Azure staging or production Azure environments (at non-peak hours) by using a third-party solution from BreakingPoint Cloud. Simulations will allow you to have a better understanding of how effective the DDoS Protection plans are and help to train your teams.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "e04bbcb8446ace6d8b36eb779972fa34",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/cdn",
    "t": "Securing CDN services ",
    "c": "Securing CDN services  Each cloud provider has its own implementation of a CDN service for distributing content closer to the customer throughout the entire world. A CDN caches content (such as images, videos, or static web pages) in multiple locations around the world, allowing customers to receive the content quickly from a location close to the customer. CDNs also serve as an extra defense mechanism against DDoS attacks by being one of the first services that serves a customer’s request, even before the request reaches the servers or applications. Best practices  Amazon Cloudfront Azure content delivery network (CDN) Google content delivery network (CDN)",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "4feaa96423f89023ecffb8671508727d",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/threat-model",
    "t": "Threat modelling ",
    "c": "Threat modelling  Threat modelling is best integrated into the design phase of an SDLC before any code is written. Threat modelling is a structured process of identifying potential security threats and prioritising techniques to mitigate attacks so that data or assets that have been classified as valuable or of higher risk during risk assessment, such as confidential data, are protected. When performed early, it brings a great advantage; potential issues can be found early and solved, saving fixing costs down the line. There are several tools and methodologies to do threat modelling. If you would like to have a DFD/threat diagram designer, you can use the Microsoft threat modeling tool , OWASP Threat Dragon , or Mozilla SeaSponge . If you have a small team and would like to do threat modelling via a card game team activity, the Microsoft EOP card game and OWASP Cornucopia are recommended. And you can add using threat libraries such as CAPEC , ATT&CK , and CWE , which can support threat identification during threat modelling. Methods  There are several methods to perform threat modelling. Not all the methods have the same purpose; focus varies from risk, to privacy concerns, to customer. The methods can be combined to understand potential threats better. STRIDE, DREAD, and PASTA fit together really well. Don’t let the process or tools limit learning and innovation. STRIDE  STRIDE stands for Spoofing, Tampering, Repudiation, Information Disclosure, Denial Of Service, and Elevation/Escalation of Privilege. STRIDE is a widely used threat model developed by Microsoft which evaluates the system’s design in a more detailed view. STRIDE can be used to identify threats, including the property violated by the threat and definition. The system’s data flow diagram is to be developed in this model, and each node is applied with the STRIDE model. Identifying security threats is a manual process that tools are not supported and should be carried out during the risk assessment. Using data flow diagrams and integrating STRIDE, the system entities, attack surfaces, like known boundaries and attacker events become more identifiable. STRIDE is built upon the CIA triad principle (Confidentiality, Integrity & Availability). Security professionals using STRIDE are looking to answer “What could go wrong with this system?” STRIDE threat Spoofing Spoofing is an act of impersonation of a user by a malicious actor which violates the authentication principle from the perspective of theCIA triad. Common ways include ARP, IP, and DNS spoofing. Tampering The modification of information by an unauthorised user. It violates the integrity principle of the CIA triad. Repudiation Not taking responsibility for events where the actions are not attributed to the attacker, violating the principle of non-repudiability. For example, the attacker clears up all the logs that could lead to leaving traces. Information Disclosure Information Disclosure is an act of violation of confidentiality of the CIA triad. A typical example is data breaches. Denial of Service Denial of Service occurs when an authorised user cannot access the service, assets, or system due to the exhaustion of network resources. DoS is a violation of the availability principle of the CIA triad. Elevation/Escalation of Privilege Escalating privileges to gain unauthorised access is a classic example of a violation of the authorisation principle of the CIA triad. DREAD  The abbreviation DREAD stands for five questions about each potential: Damage Potential, Reproducibility, Exploitability, Affected Users and Discoverability. DREAD is also a methodology created by Microsoft which can be an add-on to the STRIDE model. It’s a model that ranks threats by assigning identified threats according to their severity and priority. In other words, it creates a rating system that is scored based on risk probability. Without STRIDE, the DREAD model also can be used in assessing, analysing and finding the risk probability by threat rating. DREAD potential Damage Damage refers to the possible damage a threat could cause to the existing infrastructure or assets. It is based on a scale of 0–10. A score of 0 means no harm, 5 means Information Disclosure, 8 means user data is compromised, 9 means internal or administrative data is compromised, and 10 means unavailability of a service. Reproducibility Measures the complexity of the attack. So how easily a hacker can replicate a threat. A score of 0 means it is nearly impossible to copy, 5 stands for being complex but possible, 7.5 for an authenticated user and a score of 10 means the attacker can reproduce very quickly without any authentication. Exploitability Refers to the attack's sophistication or how easy it would be to launch the attack. A score of 2.5 implies it would require an advanced skill set of networking and programming skills; 5 means can be exploited with available tools, a score of 9 means we would need a simple web application proxy tool and a score of 10 means it can exploit through a web browser. Affected Users Describes the number of users affected by the successful exploitation of a vulnerability. A score of 0 would mean that there would be no affected users, 2.5 shall mean for an individual user, 6 would mean a small group of users, 9 would mean significant users like administrative users, and 10 would imply all users are affected. Discoverability The process of discovering the vulnerable points in the system. For example, the threat would be easily found in case of compromise. A score of 0 would mean it would be challenging to discover it, a score of 5 means that the threat can be discovered by analysis of HTTP requests, and 8 means it can be easily found as it's public-facing. A score of 10 would mean it's visible in the browser address bar. PASTA  PASTA is short for Process for Attack Simulation and Threat Analysis; it is a risk-centric threat modelling framework. PASTA’s focus is to align technical requirements with business objectives. PASTA involves the threat modelling process from analysing threats to finding ways to mitigate them, but on a more strategic level and from an attacker’s perspective. It identifies the threat, enumerates them, and then assigns them a score. This helps organisations find suitable countermeasures to be deployed to mitigate security threats. PASTA Stage Define Objectives The first stage focuses on noting the structure and defining objectives. This makes the end goal a whole lot clearer and ensures the relevant assets are threat modelled by defining an asset scope. Define Technical Scope This is where architectural diagrams are defined, both logical and physical infrastructure. Helpful in mapping the attack surface and dependencies from the environment. Decomposition & Analysis Each asset will have a defined trust boundary that encompasses all its components and data in this stage. For example, mapping threat vectors for a payment service and evaluating which components underlying the service can be leveraged for an attack; components can be libraries, dependencies, modules or underlying services etc. Threat Analysis This refers to the extracted information obtained from threat intelligence. Useful to identify which applications are vulnerable to specific vectors; for example, a customer/public-facing application can be susceptible to DDOS, unauthorised data alteration etc. Vulnerabilities & Weaknesses Analysis it analyses the vulnerabilities of web application security controls. It identifies security flaws in the application and enumerates vulnerabilities. It is highly recommended to add mitigation to the identified threat in this stage. For example, when describing a past incident involving an exploit of a mail server, lessons learned or mitigation: lack of thorough testing before implementation and hardening the server. Attack/Exploit Enumeration & Modelling In this stage, we map out the possible threat landscape and the entire...",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "5467d89c67eb43836efcf6442fd43380",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/methodologies",
    "t": "SSDLC methodologies ",
    "c": "SSDLC methodologies  Microsoft’s SDL  SDL principles: Secure by Design: Security is a built-in quality attribute affecting the whole software lifecycle. Security by Default: Software systems are constructed to minimise potential harm caused by attackers, e.g. software is deployed with the least necessary privilege. Secure in Deployment: software deployment is accompanied by tools and guidance supporting users and administrators. Communications: software developers are prepared for occurring threats by communicating openly and timely with users and administrators SDL is a collection of mandatory security activities grouped by the traditional software development lifecycle phases. Data is collected to assess training effectiveness. In-process metrics are used to confirm process compliance. Post-release metrics are used to guide future changes. SDL places heavy emphasis on understanding the cause and effect of security vulnerabilities. A development team must complete the mandatory security activities to comply with the Microsoft SDL process. OWASP’s S-SDLC  S-SDLC Principles SDL is a collection of mandatory security activities grouped by the traditional software development lifecycle phases. Data is collected to assess training effectiveness. In-process metrics are used to confirm process compliance. Post-release metrics are used to guide future changes. SDL places heavy emphasis on understanding the cause and effect of security vulnerabilities. A development team must complete the sixteen mandatory security activities to comply with the Microsoft SDL process. OWASP S-SDLC aims to build “security quality gates”, to support quality and secure software made throughout the pipeline. This is done by following an Agile Security approach, where sprints are dedicated to security. Examples of Sprints can include: Code reviews, authentication, authorisation, input validation, and assessing technical risks like code injections. The gates comprise sprints focusing on similar building blocks like those seen in Microsoft SDL. OWASP S-SDLC Agile approach is heavily influenced and based on a “Maturity Model” approach, in particular OWASP SAMM. The Software Assurance Maturity Model (SAMM)  The Software Assurance Maturity Model (SAMM) is an open framework to help organisations formulate and implement a software security strategy tailored to the organisation’s specific risks. It helps to evaluate an organisation’s existing software security practices, build a software security assurance program, demonstrate improvements to that program, and define and measure security activities for an organisation. SAMM helps explain objectives, actions, results, success metrics, costs etc. An example would be a security scorecard for gap analysis, for instance, in a particular area, like endpoint protection. It aims to answer “How well are we doing and where do we want to get to?”. Building Security In Maturity Model (BSIMM)  BSIMM is a study of real-world software security initiatives and reflects the current state of software security. BSIMM can be described as a “measuring stick” to understand your security posture by providing a comparison of other companies’ security states. In other words, it does not tell you what you should do but rather what you are doing wrong. There are hundreds of organisations involved Resources  OWASP SAMM BSIMM",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "2a8f80c6893c1f62fa1fa1218421e9a1",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/efs",
    "t": "Elastic File System (EFS) ",
    "c": "Elastic File System (EFS)  Amazon Elastic File System (Amazon EFS) is based on the NFS protocol. Authentication and authorisation  Identity and access management for Amazon EFS Working with users, groups, and permissions at the Network File System (NFS) Level AWS managed policies for Amazon EFS Security in Amazon EFS Overview of managing access permissions to your Amazon EFS resources Best practices  Avoid using the AWS root account to access AWS resources such as Amazon EFS. Create an IAM group, add users to the IAM group, and then grant the required permissions on the target Amazon EFS to the target IAM group. Use IAM roles for federated users, AWS services, or applications that need access to Amazon EFS. Use IAM policies to grant the minimal required permissions to create EFS volumes or access and use Amazon EFS. When using IAM policies, specify conditions (such as the source IP) and what actions an end user can, along with the mentioned condition, take on the target filesystem. Use resource-based policies to configure who can access the EFS volume and what actions this end user can take on the filesystem (for example, mount, read, write, and more). Network access  Amazon EFS is a managed service and located outside the customer’s VPC. Protect access to the Amazon EFS service. Controlling network access to Amazon EFS file systems for NFS clients Working with Amazon EFS Access Points Amazon Elastic File System Network Isolation Data encryption in Amazon EFS Using IAM to enforce creating encrypted file systems Best practices  Keep Amazon EFS (that is, all storage classes) private. Use VPC security groups to control the access between your Amazon EC2 machines and the Amazon EFS mount volumes. To secure access from your VPC to the Amazon EFS, use AWS PrivateLink, which avoids sending network traffic outside your VPC, through a secure channel, using an interface’s VPC endpoint. Use Amazon EFS access points to manage application access to the EFS volume. Use STS to allow temporary access to Amazon EFS. Use an IAM policy to enforce encryption at rest for Amazon EFS filesystems. You can do this by setting the value of elasticfilesystem:Encrypted to True inside the IAM policy condition. For sensitive environments, use the EFS mount helper to enforce the use of encryption in transit using TLS version 1.2 when mounting an EFS volume. Encrypt data at rest using AWS-managed CMK for Amazon EFS. For sensitive environments, encrypt data at rest using a CMK. Conducting auditing and monitoring  AWS allows enabling logging and auditing using Amazon CloudWatch and AWS CloudTrail. Logging and Monitoring in Amazon EFS Best practices  Enable Amazon CloudWatch alarms for excessive Amazon EFS usage (for example, a high volume of store or delete operations on a specific EFS volume). Enable the use of AWS CloudTrail for any EFS volume to log any activity performed on the Amazon EFS API, including any activity conducted by a user, role, or AWS service. Create a trail, using AWS CloudTrail, on any EFS volume to log events, such as a requested action, date, and time, requested parameters, and more, for access to objects stored inside AWS EFS. Limit the access to the CloudTrail logs to a minimum number of employees, preferably those with an AWS management account, outside the scope of your end users (including outside the scope of your users), to avoid possible deletion or changes to the audit logs.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "f072dde0405953d1c6eac18ee4d6c7a2",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/aks",
    "t": "Kubernetes Service (AKS) ",
    "c": "Kubernetes Service (AKS)  AKS is the Azure-managed Kubernetes orchestration service. It can integrate with other Azure services, such as ACR for storing containers, Azure AD for managing permissions to AKS, Azure Files for persistent storage, and Azure Monitor. Configuring IAM  Azure AD is the supported service for managing permissions to access and run containers through Azure AKS. AKS-managed Azure AD integration Best practices for authentication and authorisation in AKS Best practices  Enable Azure AD integration for any newly created AKS cluster. Grant minimal permissions for accessing and managing AKS, using Azure RBAC. Grant minimal permissions for accessing and managing ACR, using Azure RBAC. Create a unique service principal for each newly created AKS cluster. When passing sensitive information (such as credential secrets), make sure the traffic is encrypted in transit through a secure channel (TLS). If you need to store sensitive information (such as credentials), store it inside the Azure Key Vault service. For sensitive environments, encrypt information (such as credentials) using customer-managed keys, stored inside the Azure Key Vault service. Network access  Azure AKS exposes services to the internet – it is important to plan before deploying an Azure AKS cluster. Best practices for network connectivity and security in AKS Best practices for cluster isolation in AKS Create a private AKS cluster Best practices  Avoid exposing the AKS cluster control plane (API server) to the public internet – create a private cluster with an internal IP address and use authorised IP ranges to define which IPs can access your API server. Use the Azure Firewall service to restrict outbound traffic from AKS cluster nodes to external DNS addresses (for example, software updates from external sources). Use TLS 1.2 to control Azure AKS using API calls. Use TLS 1.2 when configuring Azure AKS behind Azure Load Balancer. Use TLS 1.2 between your AKS control plane and the AKS cluster’s nodes. For small AKS deployments, use the kubenet plugin to implement network policies and protect the AKS cluster. For large production deployments, use the Azure CNI Kubernetes plugin to implement network policies and protect the AKS cluster. Use Azure network security groups to block SSH traffic to the AKS cluster nodes, from the AKS subnets only. Use network policies to protect the access between the Kubernetes Pods. Disable public access to your AKS API server – either use an Azure VM (or Azure Bastion) to manage the AKS cluster remotely or create a VPN tunnel from the remote machine to the AKS cluster. Disable or remove the HTTP application routing add-on. If you need to store sensitive information (such as credentials), store it inside the Azure Key Vault service. For sensitive environments, encrypt information (such as credentials) using customer-managed keys, stored inside the Azure Key Vault service. Conducting auditing and monitoring  Azure allows you to enable logging and auditing using Azure Monitor for containers. ACI overview Container monitoring solution in Azure Monitor Best practices  Enable audit logging for Azure resources, using Azure Monitor, to log authentication-related activities of your ACR. Limit the access to the Azure Monitor logs to the minimum number of employees to avoid possible deletion or changes to the audit logs. Enabling compliance  Introduction to Azure Defender for Kubernetes Use Azure Defender for container registries to scan your images for vulnerabilities Azure security baseline for ACI Azure security baseline for ACR Security considerations for ACI Introduction to private Docker container registries in Azure Best practices  Use only trusted image containers and store them inside ACR – a private repository for storing your organizational images. Use Azure Defender for Kubernetes to protect Kubernetes clusters from vulnerabilities. Use Azure Defender for container registries to detect and remediate vulnerabilities in container images. Integrated Azure Container Registry with Azure Security Center to detect non-compliant images (from the CIS standard). Build your container images from scratch (to avoid malicious code in preconfigured third-party images). Scan your container images for vulnerabilities in libraries and binaries and update your images on a regular basis.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "522af34f4615a95fb68355f91d40408a",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/db-services",
    "t": "Securing managed database services ",
    "c": "Securing managed database services  To deploy a specific build of a database, it can always be deployed inside a VM, but that means overseeing the operating system and database maintenance (including hardening, backup, patch management, and monitoring). Another option is a managed solution for running the database engine: Maintenance of the database, security patch deployment, and availability of the database are under the responsibility of the cloud provider. Backups are included as part of the service (up to a certain amount of storage and amount of backup history). Encryption in transit and at rest are embedded as part of a managed solution. Auditing is embedded as part of a managed solution. The database engine can be a common database engine such as MySQL, PostgreSQL, Microsoft SQL Server, an Oracle Database server, or proprietary databases such as Amazon DynamoDB, Azure Cosmos DB, or Google Cloud Spanner. The basic idea remains the same: Select the database type according to its purpose or use case. Select a database engine. For relational databases, select a machine type (or size). Choose whether high availability is required. Deploy a managed database instance (or cluster). Configure network access control from your cloud environment to the managed database. Enable logging for any access attempt or configuration changes in the managed database. Configure backups on the managed database for recovery purposes. Connect the application to the managed database and use the service. Best practices  AWS RDS for MySQL Azure Database for MySQL Google Cloud SQL for MySQL",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "d97fcbd9f463c785cf0e9b698bdd9153",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/aad",
    "t": "Azure AD ",
    "c": "Azure AD  Azure AD supports the following types of identity models: Cloud-only identity: The user account is created inside the Azure AD tenant and only exists in the cloud (where access to on-premises is not required). Hybrid identity: The user account exists in both the Azure AD tenant and on-premises. Azure AD terminology  • User: This is a person or application with permission to access Azure resources. A user has credentials (such as a password or MFA). • Group: This is a group of users to make the permissions management task easier. • Azure role-based access control (RBAC): This is an authorization system with built-in permissions to access Azure resources (for example, Owner, Contributor, Reader, and more). Best practices securing Azure AD  • Configure a strong password for the original Azure AD global administrator account. • Enable MFA for the original Azure AD global administrator account. • Avoid using the original Azure AD global administrator account – keep the user’s password in a secured location. • Create an Azure AD user with a global administrator role for the purpose of managing the Azure AD. • Enable MFA for any user in your Azure AD (such as a global administrator role). • Limit the number of global administrators to less than five. • Use Azure RBAC to grant minimal permissions required for users to be able to access and use Azure resources. • Enable the Azure AD Privileged Identity Management (PIM) service in your Azure AD to control just-in-time access to manage Azure AD resources. • Create Azure AD groups, assign permissions to the groups, and add users to those groups for easier account and permission management. • Configure a password policy (which includes the minimum and maximum password age, the minimum password length, and enforces the use of password history and complex passwords). • Create an emergency account (these are special user accounts that shouldn’t be used daily, have MFA enabled, and are a member of the global administrator role) to avoid getting locked out of your Azure AD. • Use a conditional access policy to enforce the use of MFA and enforce the location from which a global administrator can have access to log in and manage your Azure AD (such as your office network). Best practices auditing Azure AD  • Use Azure AD Identity Protection to detect any potential vulnerability activities of your Azure AD users and send the audit logs to your SIEM system. • Use Azure AD reports to detect risky users or risky sign-in events. • Use Azure AD audit logs to review and detect events in your Azure AD (such as changes to users or groups, sign-in events, and more). • Send your Azure AD activity logs to the Azure Monitor service for further analysis. • Limit the level of access to Azure Monitor service data to a minimum number of employees, to avoid possible deletion or changes to the audit logs. • If you use the Azure AD PIM service in your Azure AD, create access reviews to detect who used PIM to request high privilege to carry out actions. • For hybrid environments (that is, an on-premises Active Directory connected to Azure AD), use Microsoft Defender for Identity to detect suspicious activities in your on-premises Active Directory.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "78443da8e4f81776c6b1cb66d418373c",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/virt",
    "t": "Securing virtual networking ",
    "c": "Securing virtual networking  Each cloud provider has its own implementation of virtual networking. According to the shared responsibility model , responsibility for the network is split between the cloud provider and the customer. The physical network layer (that is, access between the physical network equipment and physical host servers and storage inside the cloud provider’s data centers) is the cloud provider’s responsibility. Virtual networking (such as Amazon VPC, Azure VNet, or Google Cloud Platform (GCP) VPC) is a network layer that is the responsibility of the customers (this layer enables access between virtual servers, managed storage services, managed databases, and more). Traditional on-premises networking deals with the physical connections between devices in a system: for example, concepts such as virtual local area networks (VLANs) or subnetting, to split a network (with the devices connected to a network) and create network security barriers. In the cloud, a network is software-based (software-defined networking (SDN)). In the cloud, you have micro-segmentation, which means you can configure allow and deny access control rules between two instances (even if they are located on the same subnet). You will also be able to audit and control access to a resource such as an API. A virtual network is a network area inside a cloud environment where most of the common cloud resources reside (such as virtual servers, managed databases, and so on). These resources are split into multiple network segments called subnets. There can be one or more virtual networks in each customer’s cloud environment, but at the end of the day, the basic idea is the same: Access to subnets is controlled by access controls (such as Layer 4 firewalls). Subnets can be private (no direct access from the internet) or public (access from the internet is allowed). If you need to allow access to the internet for private subnet resources, you need to configure a NAT gateway. Virtual networks can be connected with each other via peer connections. Best practices  Amazon Virtual Private Cloud (VPC) Azure Virtual Network (VNet) Google Cloud VPC",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "93725bb1501385c67b003617e5b4e533",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/waf",
    "t": "AWS WAF ",
    "c": "AWS WAF  AWS WAF offers protection against the following types of attacks: Layer 7 DDoS attacks (when combined with AWS Shield) Common web application attacks Bots (non-human generated traffic) AWS WAF also allows you to protect the following Amazon services: Amazon CloudFront: The Amazon managed CDN service Amazon API Gateway: The Amazon managed API gateway service Amazon ALB: The Amazon managed Application Load Balancer service (Layer 7 load balancer) Best practices  To protect an external web resource, create web ACLs, with either allow or block actions. When creating a new web ACL rule, change the default CloudWatch metric name to an informative name that will allow you to detect it later when reviewing the CloudWatch logs. Use Amazon CloudWatch to monitor your web ACL rule activity. For protection against non-standard types of web application attacks, create your own custom rules. For better protection, subscribe to the rules available on the AWS Marketplace (created by security vendors). For large-scale environments with multiple AWS accounts, use AWS Firewall Manager to centrally create and enforce WAF rules. Send AWS WAF logs to the Amazon Kinesis Data Firehose service to review near real-time logs of attacks. Use AWS Config to enable logging for every newly created web ACL. Use AWS IAM to limit the permissions to the AWS WAF Console. Use AWS CloudTrail to log actions in the AWS WAF Console.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "3c0ac60ad697165aa1c2796663a66c02",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/functions",
    "t": "Securing serverless/function as a service ",
    "c": "Securing serverless/function as a service  The term serverless or function as a service means that customers of the service, are not in charge of the underlying infrastructure, and just import code (according to the supported language by each cloud provider), select preferred runtime, select amount of required memory per function (affecting the amount of CPU), and set a trigger to invoke the function. Best practices  AWS Lambda Azure Functions Google Cloud Functions",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "9dab1ba7248f8eb1dd75a2dfa3823237",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/readme",
    "t": "Introduction ",
    "c": "Introduction  What?  Securing GCP services. Why?  To be able to support the desperate better. How?  Compute Engine (GCE) and VM instances SQL for MySQL Kubernetes Engine (GKE) Functions Storage Permanent Disk Filestore Container Storage Interface (CSI) Virtual Private Cloud (VPC) Managed DNS Content delivery network (CDN) Managed VPN Google Cloud IAM",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "c27883c625bf25c95ba496c1895ad980",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/files",
    "t": "Files ",
    "c": "Files  Azure Files is an Azure file storage service based on the SMB protocol. Authentication and authorisation  Azure supports Active Directory Domain Services (AD DS) and Azure Active Directory Domain Services (Azure AD DS), an add-on service to Azure AD, which allows for authenticating to legacy services (SMB in the case of Azure Files) and protocols (Kerberos in the case of Azure Files). Overview of Azure Files identity-based authentication options for SMB access Planning for an Azure Files deployment Best practices  Use identity-based authentication to grant minimal permissions for sharing, directory, or file level access on the Azure Files service. For a cloud-native environment in Azure (with no on-premises VMs), make sure all VMs are joined to an Azure AD domain and that all VMs are connected to the same VNet as Azure AD DS. Enable Active Directory authentication over SMB to allow domain joined VMsaccess to Azure Files. Avoid using storage account keys for authenticating to Azure Files. Network access  Azure Files is a managed service, and located outside the customer’s VNet. Protect access to the Azure Files service. Azure Files networking considerations Prevent accidental deletion of Azure file shares Require secure transfer to ensure secure connections Enforce a minimum required version of Transport Layer Security (TLS) for requests to a storage account Azure Storage encryption for data at rest Customer-managed keys for Azure Storage encryption Best practices  Since SMB is considered a non-secure protocol, make sure all access to Azure Files services from the on-premises network pass through a secured channel such as a VPN tunnel or an ExpressRoute service. To secure access from your VNet to Azure Files, use an Azure private endpoint, which avoids sending network traffic outside your VNet, through a secure channel. Remove the need for the use of transport encryption (HTTPS only) for all Azure Files shares. For sensitive environments, require a minimum TLS version of 1.2 for Azure Blob storage. Deny default network access to the Azure storage account and only allow access from a predefined set of IP addresses. For data that you need to retain for long periods (due to regulatory requirements), enable the Azure Files soft delete feature to protect the data from accidental deletion. Encrypt data at rest using Azure Key Vault. For sensitive environments, encrypt data at rest using customer-managed keys stored inside Azure Key Vault. Auditing and monitoring  Azure allows monitoring Azure Files using Azure Monitor and Advanced Threat Protection for Azure Storage. Requests logged in logging Monitoring Azure Files Best practices  Enable log alerts using the Azure Monitor service to track access to Azure Files and raise alerts (such as multiple failed access attempts to Azure Files in a short period of time). Enable Azure Defender for Storage to receive security alerts inside the Azure Security Center console. Enable Azure storage logging to audit all authorisation events for access to the Azure storage.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "9db1f8392b1bf7803dfe7ea47ebf64f3",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/gce",
    "t": "Compute Engine (GCE) and VM instances ",
    "c": "Compute Engine (GCE) and VM instances  Use only trusted images when deploying Google VMs. Use a minimal number of packages inside an image, to lower the attack surface. Use GCP built-in agents for Google VMs (patch management, hardening, monitoring, and so on). For highly sensitive environments, use Google Confidential Computing images , to ensure security and isolation of customers’ data. Authenticating to a VM instance  Google does not have access to customers’ VM instances. When you run the create instance wizard, no credentials are generated. For Linux instances, you can choose from several access methods . Windows virtual machine (VM) instances authenticate by using a username and a password instead of by using SSH . You can enable SSH . Best practices  Keep private keys in a secured location. Avoid storing private keys on a bastion host (machine instances directly exposed to the internet). Periodically rotate SSH keys used to access compute instances. Periodically review public keys inside the compute instance or GCP project-level SSH key metadata and remove unneeded public keys. Join Windows or join Linux instances to an AD domain and use your AD credentials to log in to the VMs (and avoid using local credentials or SSH keys completely). Network access to a VM instance  Access to GCP resources and services such as VM instances is controlled via VPC firewall rules , which are equivalent to the on-premises layer 4 network firewall or access control mechanism. Best practices  For remote access protocols (SSH/RDP), limit the source IP (or CIDR) to well-known addresses. For file sharing protocols (CIFS/SMB/FTP), limit the source IP (or CIDR) to well-known addresses. Set names and descriptions for firewall rules to allow a better understanding of the security group’s purpose. Use tagging (labelling) for firewall rules to allow a better understanding of which firewall rule belongs to which GCP resources. Limit the number of ports allowed in a firewall rule to the minimum required ports for allowing your service or application to function. Serial console connection  For troubleshooting purposes, you can connect using a serial console to resolve network or operating system problems when SSH or RDP connections are not available. To allow serial access on the entire GCP project (using the Google Cloud SDK): gcloud compute project-info add-metadata \\ --metadata serial-port-enable=TRUE This exposes the VM instance. Best practices  Configure password-based login to allow users access to the serial console. Disable interactive serial console login per compute instance when not required. Enable disconnection when the serial console connection is idle. Access to the serial console should be limited to the required group of individuals using Google Cloud IAM roles. Always set a user password on the target VM instance before allowing access to the serial console. Patch management  To deploy security patches for either Windows- or Linux-based instances, in a standard manner, it is recommended to use Google operating system patch management : Deploy the operating system config agent on the target instances. Create a patch job . Run patch deployment. Schedule patch deployment. Review the deployment status inside the operating system patch management dashboard. Best practices  Use minimal privileges for the accounts using operating system patch management to deploy security patches, according to Google Cloud IAM roles. Gradually deploy security patches zone by zone and region by region . Use tagging (labelling) for VM instances to allow defining groups of VM instances (for example, production versus development environments). For stateless VMs (where no user session data is stored inside a Google VM), replace an existing Google VM with a new instance, created from an up-to-date operating system image.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "83edba51d8f98cef9c32100487ac1568",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/implement",
    "t": "Implementing SSDLC ",
    "c": "Implementing SSDLC  Secure SDLC involves instilling security processes at all lifecycle phases. From security testing tools to writing security requirements alongside functional requirements. Security posture  Understanding the gaps and current state is critical for successfully introducing a new tool, solution, or change. To help grasp what the current security posture is, start by doing the following: Perform a gap analysis to determine what activities and policies exist in the organisation and how effective they are. For example, ensuring policies are in place (what the team does) with security procedures (how the team executes those policies). Create Software Security Initiatives (SSI) by establishing realistic and achievable goals with defined metrics for success. For example, this could be a Secure Coding Standard, playbooks for handling data, etcetera are tracked using project management tools. Formalise processes for security activities within your SSI. After starting a program or standard, it is essential to spend an operational period helping engineers get familiarised with it and gather feedback before enforcing it. When performing a gap analysis, every policy should have defined procedures to make them effective. Invest in security training for engineers as well as appropriate tools. Ensure people are aware of new processes and the tools that will come with them to operationalise them, and invest in training early, ideally before using the tool. SSDLC processes  A secure SDLC involves integrating processes like security testing and other activities into an existing development process: Risk Assessment - during the early stages of SDLC, it is essential to identify security considerations that promote a security by design approach when functional requirements are gathered in the planning and requirements stages. For example, if a user requests a blog entry from a site, the user should not be able to edit the blog or remove unnecessary input fields. Threat Modelling - is the process of identifying potential threats when there is a lack of appropriate safeguards. It is very effective when following a risk assessment and during the design stage of the SDLC, as Threat Modelling focuses on what should not happen. In contrast, design requirements state how the software will behave and interact. For example, ensure there is verification when a user requests account information. Code Scanning / Review - Code reviews can be either manual or automated. Code Scanning or automated code reviews can leverage Static and Dynamic Security testing technologies. These are crucial in the Development stages as code is being written. Security Assessments - Like Penetration Testing, Vulnerability Assessments are a form of automated testing that can identify critical paths of an application that may lead to exploitation of a vulnerability. However, these are hypothetical as the assessment doesn’t carry simulations of those attacks. Pentesting, on the other hand, identifies these flaws and attempts to exploit them to demonstrate validity. Pentests and Vulnerability Assessments are carried out during the Operations & Maintenance phase of the SDLC after a prototype of the application.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "6dbbba9a3a53a66004497a56ef0c0b03",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/file",
    "t": "Securing file storage ",
    "c": "Securing file storage  File storage is a piece of storage such as the on-premises network-attached storage (NAS). In general, file storage: offers support for common file sharing protocols (such as NFS and SMB/CIFS). can mount a volume from a managed file service into an operating system to store and retrieve files, for multiple VMs, in parallel. can control access permissions to the remote filesystem. enables automatic filesystem growth. Best practices  Amazon Elastic File System (EFS) Azure Files Google Filestore",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "4cd994a49c35dba0131469d024bdf93f",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/readme",
    "t": "Introduction ",
    "c": "Introduction  What?  Securing AWS services. Why?  To be able to support the desperate better. How?  Elastic Compute Cloud (EC2) RDS for MySQL Elastic Container Service (ECS) Elastic Kubernetes Service (EKS) AWS Lambda Simple Storage Service Elastic Block Store (EBS) Elastic File System (EFS) Container Storage Interface (CSI) Virtual Private Cloud (VPC) Route 53 CloudFront Site-to-Site VPN Client VPN AWS Shield AWS WAF IAM Directory Service Configuring MFA",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "7164b9207d6c21e95cbe30229b48ee94",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/vpn",
    "t": "Managed VPN ",
    "c": "Managed VPN  Google Cloud VPN is a managed service that allows one to connect a corporate network to the GCP environment in a secure channel (using an IPSec tunnel). Best practices  Restrict access to GCP resources inside your Google Cloud VPC using VPC firewall rules. Use the AES-GCM-16-256 algorithm for both encryption of the IPSec tunnel and ensuring the integrity of the traffic passing through the tunnel. Use a strong, 32-character, pre-shared key to authenticate to the Google Cloud VPN tunnel. Create an IAM group, add users to the group, and grant the required permissions on the Google Cloud VPN connection for the target group. Use Google Cloud Logging to monitor activity on the Google Cloud VPN service.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "bca0f07337738d53fc66292a342a2334",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/block",
    "t": "Securing block storage ",
    "c": "Securing block storage  Block storage is a storage scheme like an on-premises Storage Area Network (SAN). It allows mounting a volume (disk), formatting it to a common filesystem (such as NTFS for Windows or Ext4 for Linux), and storing various files, databases, or entire operating systems. Best practices  Amazon Elastic Block Store Azure managed disks Google Persistent Disk",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "17334bcebc437439aeff55dbbf9a2ba7",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/storage",
    "t": "Storage ",
    "c": "Storage  Google Cloud Storage is the GCP object storage service. Authentication and authorisation  Access can be controlled at the entire bucket level (including all objects inside this bucket) or on a specific object level (for example, suppose you would like to share a specific file with several of your colleagues). Identity and Access Management Cloud Storage authentication Access control lists (ACLs) Retention policies and retention policy locks Security Token Service API HMAC keys Signed URLs 4 best practices for ensuring privacy and security of your data in Cloud Storage Best practices  Create an IAM group, add users to the IAM group, and then grant the required permissions on the target cloud storage bucket to the target IAM group. Use IAM policies for applications that require access to cloud storage buckets. Grant minimal permissions to cloud storage buckets. Use Security Token Service (STS) to allow temporary access to cloud storage. Use HMAC keys to allow the service account temporary access to cloud storage. Use signed URLs to allow an external user temporary access to cloud storage. For data that you need to retain for long periods (due to regulatory requirements), use the bucket lock feature to protect the data from accidental deletion. Network access  Because Google Cloud Storage is a managed service, it is located outside the customer’s VPC. It is important to protect access to Google Cloud Storage. Security, ACLs, and access control The security benefits of VPC Service Controls Enabling VPC accessible services Customer-supplied encryption keys Customer-managed encryption keys Best practices  Use TLS for transport encryption (HTTPS only). Keep all cloud storage buckets (all tiers) private. Use VPC Service Controls to allow access from your VPC to Google Cloud Storage. Encrypt cloud storage buckets using Google-managed encryption keys inside Google Cloud KMS. For sensitive environments (for example, which contain PII, credit card information, healthcare data, and more), encrypt cloud storage buckets using a CMK inside Google Cloud KMS. Auditing and monitoring  GCP allows you to enable logging and auditing using Google Cloud Audit Logs. Cloud Audit Logs with Cloud Storage Usage logs & storage logs Best practices  Admin activity audit logs are enabled by default and cannot be disabled. Explicitly enable Data Access audit logs to log activities performed on Google Cloud Storage. Limit the access to audit logs to a minimum number of employees to avoid possible deletion or any changes made to the audit logs.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "df85b262053bf7dda5d83aa88b8c1502",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/disk",
    "t": "Persistent Disk ",
    "c": "Persistent Disk  Google Persistent Disk is a part of the GCP block storage. To encrypt a persistent disk, in a specific GCP project, in a specific region, using a specific encryption key: gcloud compute disks \\ create encrypted-disk \\ --kms-key \\ projects/[KMS_PROJECT_ID]/locations/[REGION]/ keyRings/[KEY_RING]/cryptoKeys/[KEY] Best practices  Encrypt both the OS and data volumes. Encrypt each data volume at creation time. Encrypt the machine instance snapshots. For highly sensitive environments, encrypt persistent disks using a CMK inside Google Cloud KMS. Set names for Google’s persistent disks to allow you to have a better understanding of which persistent disk belongs to which machine instance. Use tagging (labelling) for persistent disks or snapshots for a better understanding of which disk or snapshot belongs to which machine instance.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "83d96cb1e583d3a1d298735a9541f141",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/story",
    "t": "The story sofar ",
    "c": "The story sofar  Understanding how things have improved over the years and their impact on the security industry. DevOps  In 2008, a conversation between Andrew Clay and Patrick Debois led to something quite revolutionary. While discussing the drawbacks of Agile, DevOps was born. After an event in Belgium the following year called “DevOpsDays,” DevOps became the next buzzword, and its popularity increased. DevOps is quite different from other methodologies because it focuses on driving ”cultural change” to increase efficiency. It attempts to unite the magic of all teams working on a project, using integration and automation. With these ingredients, you get a cross-integration across all departments, QA+sysadmins+developers. For example, ensuring developers can now be involved in deployment and sysadmins can now write scripts, QA can figure out how to fix flaws vs constantly testing for functionality. By introducing automation and integration of systems, these engineers can now have the same visibility at all times and interact accordingly. We will dive more into how DevOps does this in the latter rooms as we talk about pipelines, automation, Continuous Integration and Continuous Delivery (CI/CD). DevOps builds a philosophy that emphasises building trust and better liaising between developers and other teams (sysadmins, QA, etc.). This helps the organisation align technological projects to business requirements, increasing the impact and value to the business as projects become more efficient and prioritised accordingly. Changes rolled out are usually small and reversible, visible to all teams involved. This ensures better contribution and communication that helps with the pace and an increased competency when delivering work. TL;DR  Development infrastructure can be fully automated and operate on a self-service basis: Developers can provide resources to public clouds without depending on IT to provision infrastructure, which in the past led to weeks to months of delays. Continuous integration and deployment (CI/CD) processes automatically set up testing, staging, and production environments in the cloud or on-premises. They can be decommissioned, scaled, or re-configured as needed. Infrastructure-as-Code (IaC) is widely used to deploy environments declaratively*, using tools like Terraform and Vagrant. Organisations can now provision containerised workloads dynamically using automated, adaptive processes. The declarative approach requires that users specify the end state of the infrastructure - for example, deploy these machines in a running state directly into an environment, automating the configuration choices throughout the workflow. The software builds it and releases it with no human interaction. The imperative/procedural approach takes action to configure systems in a series of actionable steps. For example, you might declare to deploy a new version of the software and automate a series of steps to get a deployment-ready state. You choose when to apply those changes at the end by adding a “gate” this gate could be a button to release the changes, e.g. “deploy changes” button, after all the automated checks and new configurations pass. In such a workflow, even a tiny problem could create a mess. Moreover, as the number of new releases increases (the actual case), the whole matter may turn disastrous. Things would surely go out of hand with an issue still unresolved and plenty of features scheduled to be released. The infinite loop  DevOps is visualised as an infinite loop, describing all the comprising phases: CI/ CD – Continuous Integration and Continuous Deployment (CI/CD) deals with the frequent merging of code and adding testing in an automated manner to perform checks as new code is pushed and merged. We can test code as we push and merge thanks to a new dynamic and routine in deployment, which takes the form of minor code changes systematically and routinely. Thanks to this change in dynamic, CI/CD helps detect bugs early and decreases the effort of maintaining modular code massively, which introduces reliable rollbacks of versions/code. INFRASTRUCTURE AS CODE (IaC) – a way to manage and provision infrastructure through code and automation. Thanks to this approach, we can reuse code used to deploy infrastructure (for example, cloud instances), which helps inconsistent resource creation and management. Standard tools for IaC are terraform, vagrant, etc. We will use these tools further in the pathway as we experiment with IaC security. CONFIGURATION MANAGEMENT – This is where the state of infrastructure is managed constantly and applying changes efficiently, making it more maintainable. Thanks to this, lots of time is saved, and more visibility into how infrastructure is configured. You can use IaC for configuration management. ORCHESTRATION – Orchestration is the automation of workflows. It helps achieve stability; for example, by automating the planning of resources, we can have fast responses whenever there is a problem (e.g., health checks failing); this can be achieved thanks to monitoring. MONITORING – focuses on collecting data about the performance and stability of services and infrastructure. This enables faster recovery, helps with cross-team visibility, provides more data to analyze for better root-cause analysis, and also generates an automated response, as mentioned earlier. MICROSERVICES – An architecture that breaks an application into many small services. This has several benefits, like flexibility if there is a need to scale, reduced complexity, and more options for choosing technology across microservices. We will look at these in more detail in the DevSecOps pathway. DevSecOps  Security can supposedly be easily integrated because of the visibility and flexibility that DevOps introduces. “Shifting Left” means that DevOps teams focus on instilling security from the earliest stages in the development lifecycle and introducing a more collaborative culture between development and security. Since security can now be introduced early, risks are reduced massively. In the past, you would find out about security flaws and bugs at the very late stages, even in production. They are leading to stress, rollbacks, and economic losses. Integrating code analysis tools and automated tests earlier in the process can now identify these security flaws during early development. Shifting left  In the past, security testing was implemented at the end of the development cycle. As the industry evolved and security functions were introduced, security teams would perform various analyses and security testing in the final stages of the lifecycle. Depending on the results of security testing, it would either permit the application to proceed for deployment into production or reject the application and pass it back to developers for remediating the flaws identified. This resulted in long delays in development and friction between teams. Implementing security measures during all stages of the development lifecycle (shifting left) rather than at the end of the cycle will ensure the software is designed with security best practices built in. By detecting security flaws early in development, remediation costs are lower, and there would be no need to roll back changes as they are being addressed on time. This reduces cost, builds trust, and improves the security and quality of the product. Resources  History of DevOps Devopedia: Shift Left 3 DevSecOps success stories",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "673314543b0fbc43480f5c02abfab506",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/rds-mysql",
    "t": "RDS for MySQL ",
    "c": "RDS for MySQL  Configuring IAM  MySQL supports the following types of authentication methods: Local username/password authentication against MySQL’s built-in authentication mechanism. AWS IAM database authentication. AWS Directory Service for Microsoft AD authentication. Best practices  For the local MySQL default user, create a strong and complex password, and keep the password in a secured location. For end users who need direct access to the managed database, the preferred method is to use the AWS IAM service. If you manage user identities using AWS Directory Service for Microsoft AD (AWS-managed Microsoft AD), use it to authenticate end users using the Kerberos protocol . Securing network access  Access to a managed MySQL database service is controlled via database security groups, which are equivalent to security groups and the on-premises layer 4 network firewall or access control mechanism. Best practices  Managed databases must never be accessible from the internet or a publicly accessible subnet – always use private subnets to deploy databases. Configure security groups for web or application servers and set the security group as target CIDR when creating a database security group. If you need to manage the MySQL database service, either use an EC2 instance (or bastion host) to manage the MySQL database remotely or create a VPN tunnel from your remote machine to the managed MySQL database. Since Amazon RDS is a managed service, it is located outside the customer’s VPC. An alternative to secure access from your VPC to the managed RDS environment is to use AWS PrivateLink , which avoids sending network traffic outside your VPC, through a secure channel, using an interface VPC endpoint. Set names and descriptions for the database security groups to allow a better understanding of the database security group’s purpose. Use tagging (labelling) for database security groups to allow a better understanding of which database security group belongs to which AWS resources. Stored data  To protect customers’ data, encrypt data both in transport and at rest. Best practices  Enable SSL/TLS 1.2 transport layer encryption to the database. Select the right encryption oprions : For non-sensitive environments, encrypt data at rest using AWS KMS. For sensitive environments, encrypt data at rest using customer master key (CMK) management. Conducting auditing and monitoring  As with any other managed service, AWS allows for logging and auditing using two built-in services: Amazon CloudWatch: A service that allows logging database activities and raise an alarm according to predefined thresholds. AWS CloudTrail : A service that allows monitoring API activities (any action performed as part of the AWS RDS API). Best practices  Enable Amazon CloudWatch alarms for high-performance usage (which may indicate anomalies in the database behaviour). Enable AWS CloudTrail for any database, to log any activity performed on the database by any user, role, or AWS service. Limit access to the CloudTrail logs to the minimum number of people – preferably in an AWS management account, outside the scope of your end users (including outside the scope of your database administrators), to avoid possible deletion or changes to the audit logs.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "ae4f936e9dfeb7ea92b537a7c1ffe799",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/pia",
    "t": "Privacy Impact Assessment (PIA) ",
    "c": "Privacy Impact Assessment (PIA)  The objective of a PIA is to perform an initial self-assessment of what business modules may involve privacy data handling and readiness for GDPR compliance. the data privacy impact analysis is required by the GDPR article 35. Privacy data attributes  Attributes Related business flow or applications Privacy data type Describe collected or processed privacy data, such as name, address, phone Purpose of colection Describe the objective of the data collection and the business Is it a must? Is the data collection essential to keep the business application running? Ways of collection How the personal data is collected, such as API, email, or web form registration Lawful basis Is the data collection based on user agreement, contract, or legal compliance? Rights of data subject Can the data subject edit or delete the data? Transmission How the data is transmitted, such as FTP, email, or API Storage country Which country is the data stored in? Storage format In what format is the data stored, such as big data, relational database, or paper-based? Expiration period Any specified expiration period of the data usage? Cross-border transfer Will the data be transferred out of or into the EU? Third-party involvement Is any third party involved with the data processing? Owner Who/which team is the owner of the data? GDPR security requirements  A controller is the entity that determines the purposes, conditions and means of processing of personal data, while the processor is an entity which processes personal data on behalf of the controller. Requirement Processor Controller Provide Data Privacy Declaration Must Must Data collection requires a user's explicit consent to data collection and allows a user to disable data collection. Must Must For the purpose of error troubleshooting, the user must be informed if the collectionof logs includes personal information. Must Must Collection of a user's cookies requires the user's consent. Must Must If the data is collected for marketing analysis purposes, the application must allow users to disable the analysis. Recommended Must Provide a secure data removal capability after the data expires. Must Must If the data will be provided to third-party partners, it must have the user's explicit consent. Recommended Must Provide the capability for a user to query and update the data. Recommended Must Delete any temporary data which is no longer in use. Recommended Must Provide the capability to export the data. Recommended Must Secure data transmission. Must Must Secure local data storage with encryption, access control, and logging security controls. Must Must",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "3ba297c8c0a84ba46ee9399199d8e7df",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/vnet",
    "t": "Virtual Network (VNet) ",
    "c": "Virtual Network (VNet)  Network access  To protect resources inside a VNet, Azure supports network security groups (NSGs) – a stateless mechanism for protecting access to resources, at a virtual machine, subnet, or tagging level. You need to configure both inbound and outbound rules for VNet NSGs. Best practices  Create subnets according to the resource function (for example, public subnets for web servers, private subnets for database servers, and so on). For remote access protocols (SSH/RDP), limit the source IP address (or CIDR) to well-known sources. For file sharing protocols (CIFS/SMB/FTP), limit the source IP address (or CIDR) to well-known sources. Use NSGs to control access between public resources (such as load balancer or publicly facing web servers) and private resources (such as databases) and limit the access to the minimum required ports/protocols. Set names and descriptions for NSGs to allow a better understanding of any NSG’s purpose. Use tagging (also known as labeling) for NSGs to allow a better understanding of which network security groups belong to which Azure resources. Use application security groups to define access rules at the application layer (for example, rules for allowing inbound HTTP access). Use service tags to configure rules for pre-defined service (for example, Azure Load Balancer), instead of using IP addresses. To allow outbound access from internal resources inside private subnets to destinations on the internet, use Azure Virtual Network NAT (or use a NAT gateway). For large-scale environments with multiple Azure subscriptions, use Azure Firewall to centrally create, enforce, and log network policies across multiple subscriptions. Monitoring  Azure allows you to monitor network activity using Azure Network Watcher – a service for capturing NSG Flow Logs. Best practices  Enable Azure Network Watcher NSG Flow Logs to log and further analyze allowed and denied traffic activity. If you need to troubleshoot network issues by capturing network traffic, use the Azure Network Watcher packet capture extension to copy live network traffic from a virtual machine to an Azure storage account. For large-scale environments, use Azure Traffic Analytics to locate security threats (such as open ports, application outbound internet traffic, and so on). Use Azure Policy to detect inbound access to resources inside your VNet via unencrypted protocols (such as HTTP instead of HTTPS, or LDAP instead of LDAPS). For large-scale production environments, enable the Network Watcher packet capture extension only for short periods of time, for troubleshooting purposes only (due to the performance impact on the target VM). For large-scale production environments, enable NSG Flow Logs only for short periods of time, for troubleshooting purposes only (due to high storage cost and large amounts of data generated by NSG Flow Logs).",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "ee301e8c3d10f87de803a9226bc504c8",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/vpc",
    "t": "Virtual Private Cloud (VPC) ",
    "c": "Virtual Private Cloud (VPC)  Network access  AWS supports Network ACLs and Security groups to protect access to resources inside a VPC. Network access to a resource is granted by a combination of the network ACLs on a subnet level with an aggregation of all the security groups, effective on a resource such as a virtual machine (in case several security groups allow different access to a resource such as a virtual machine). Best practices  When creating custom network ACLs, create a final deny rule for both inbound and outbound traffic for better protection. Create subnets according to the resource’s function (for example, public subnets for web servers, private subnets for database servers, and so on). For remote access protocols (SSH/RDP), limit the source IP address (or Classless Inter-Domain Routing (CIDR)) to well-known sources. For file sharing protocols (CIFS/SMB/FTP), limit the source IP address (or CIDR) to well-known sources. Use security groups to control access between public resources (such as load balancers or publicly facing web servers) and private resources (such as databases) and limit the access to the minimum required ports/protocols. Set names and descriptions for security groups to allow a better understanding of any security group’s purpose. Use tagging (also known as labeling) for security groups to allow a better understanding of which security groups belong to which AWS resources. For large-scale environments with multiple AWS accounts, use AWS Firewall Manager to centrally create and enforce VPC security groups. For secure access from resources inside your VPC to AWS Managed Services (such as AWS S3, Amazon RDS and more), and to keep traffic inside the AWS backbone, use AWS PrivateLink, and configure your VPC security groups to allow traffic from your VPC to AWS managed services. To allow outbound access from internal resources inside private subnets to destinations on the internet (based on the IPv4 protocol), use NAT gateways or any self-hosted NAT proxy. To allow outbound access from internal resources inside private subnets to destinations on the internet (based on the IPv6 protocol), use an egress-only internet gateway. Monitoring  AWS allows for monitoring Amazon VPC using Amazon CloudWatch and VPC Flow Logs. Best practices  Enable CloudWatch Logs to monitor your VPC components’ activity and the traffic between your VPC resources and the VPC endpoint (AWS managed services). Use AWS CloudTrail to monitor VPC configuration. Enable VPC Flow Logs to log and further analyze allowed and denied traffic activity. Combined with Amazon GuardDuty, you will be able to detect anomalous network behaviour, such as interaction with command and control (C&C) networks, malicious IP addresses, and more. Use AWS Config or AWS Security Hub to detect inbound access to resources inside your VPC via unencrypted protocols (such as HTTP instead of HTTPS, or LDAP instead of LDAPS). In case you need to troubleshoot network issues by capturing network traffic without interrupting production systems, use Traffic Mirroring in Amazon VPC to copy live network traffic from a network interface of an Amazon Elastic Compute Cloud (EC2) instance, or from a network load balancer to an out-of-band security appliance. For large-scale production environments, enable VPC Flow Logs only for short periods of time, for troubleshooting purposes only (due to high storage cost and large amounts of data generated by VPC Flow Logs).",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "45d9a3a47e37ac2439a56d08e2571898",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/cloudfront",
    "t": "CloudFront ",
    "c": "CloudFront  Best practices  Restrict access to origin servers (where your original content is stored) from CDN segments only (allow traffic only from the CDN segments towards servers or services that store content). Share content via the HTTPS protocol to preserve the confidentiality of the content and to assure the authenticity of the content. When distributing content over HTTPS, use TLS 1.2 over older protocols, such as SSL v3. If you have a requirement to distribute private content, use CloudFront signed URLs. If you have a requirement to distribute sensitive content, use field-level encryption as an extra protection layer. Use AWS Web Application Firewall (WAF) to protect content on Amazon CloudFront from application-layer attacks (such as detecting and blocking bot traffic, OWASP Top 10 application attacks, and more). Enable CloudFront standard logs for audit logging purposes. Store the logs in a dedicated Amazon S3 bucket, with strict access controls, to avoid log tampering.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "46bbc27879abd383f689affcef29b0e8",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/p2s-vpn",
    "t": "Point-to-Site VPN ",
    "c": "Point-to-Site VPN  Azure VPN Gateway (Point-to-Site) allows connection from anywhere on the internet to the Azure environment in a secure channel using an OpenVPN client, Secure Socket Tunneling Protocol (SSTP), or Internet Key Exchange version 2 (IKEv2) VPN client. Best practices  If you are managing user identities inside Azure Active Directory, use Azure Active Directory to authenticate users to the VPN gateway, combined with Azure RBACs to provide minimal access to resources on your Azure environment. If authenticating users through Azure Active Directory, enforce MFA for your end users. For highly sensitive environments, use certificates to authenticate to the point-to-site VPN tunnel. Restrict access to Azure resources inside your Azure environment using NSGs. Use the GCMAES256 algorithm for both encryption of the IPSec tunnel and ensuring the integrity of the traffic passing through the tunnel. Use Azure Monitor to monitor the VPN tunnels (for example, it could send alerts when the amount of traffic in bytes is above a pre-defined threshold) and to log audit-related events (an example of suspicious behaviour could be a user attempting to log-in in the middle of the night for the first time). Enable Azure DDoS Protection to protect your VPN gateway from DDoS attacks. Use Azure Advanced Threat Protection to identify anomalous behaviour of users connecting through the point-to-site VPN tunnel. Use Azure Security Center to detect security-related events through the VPN gateway.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "21db6b125e9750723d4941ba456b134d",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/shield",
    "t": "Shield ",
    "c": "Shield  AWS Shield is the Amazon managed DDoS protection service and comes in two price models: AWS Shield Standard: This is the default and free Layer 7 DDoS protection (HTTP/HTTPS), provided for all customers. AWS Shield Advanced: This offers Layers 3/4 (Network layer) and Layer 7 (Application layer) DDoS protection, with additional protection for AWS services such as DNS (Route 53), CDN (CloudFront), Elastic Load Balancing (ELB), and virtual machine (EC2) services. This price tier also offers support from the AWS DDoS response team. Best practices  Use AWS Shield Standard for any web-based production environment you expose to the internet. Use AWS Shield Advanced for large-scale production environments you expose to the internet for better insights into the attacks. When using AWS Shield Advanced, register an Elastic IP (EIP) address as a protected source to allow quicker detection of attacks. When using AWS Shield Advanced, you can generate near real-time reports about attacks on resources in your AWS account(s). When combining AWS Shield and the AWS WAF service, use Amazon CloudWatch to monitor incoming requests and alert you when there is a spike in incoming requests, to have preliminary alerts on incoming DDoS attacks. Use AWS Identity and Access Management (IAM) to limit the permissions to the AWS Shield Console. Use AWS CloudTrail to log actions in the AWS Shield Console.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "5121b8fb093ef0628f1d5c400be6a467",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/csi",
    "t": "Container Storage Interface (CSI) ",
    "c": "Container Storage Interface (CSI)  Google Kubernetes Engine has a CSI driver for the following storage types: Block storage: Google Compute Engine Persistent Disk Object storage: Google Cloud Storage Managed NFS: Google Cloud Filestore Best practices  Always use the latest CSI version for your chosen storage type. When using the CSI driver for Google Persistent Disk, specify (in the YAML file) the disk-encryption-kms-key key to allow the CSI driver to use a customer-managed encryption key from Google Cloud KMS. Use Cloud IAM roles to restrict access from your GKE cluster to Google Cloud Filestore. Use Google Secret Manager with the Secret Store CSI driver to store and retrieve secrets (such as tokens, SSH authentication keys, Docker configuration files, and more) to/from your GKE pods.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "1d901538c23d75fe577cb2fe8468bfd9",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/functions",
    "t": "Functions ",
    "c": "Functions  Azure Functions is the Azure function as a service. It can integrate with other Azure services, such as Azure AD for managing permissions to Azure Functions, Azure Monitor Application Insights for monitoring Azure Functions, and Azure Blob storage for persistent storage. Configuring IAM  Azure AD is the supported service for managing permissions to your Azure Functions. How to use managed identities for App Service and Azure Functions Azure Functions authorisations Use Key Vault references for App Service and Azure Functions Best practices  Enable Azure AD authentication for any newly created Azure function by turning on Azure App Service authentication. Avoid allowing anonymous access to your Azure function – require clients to authenticate before using Azure Functions. Grant minimal permissions for any newly created Azure function using Azure RBAC. Prefer to use temporary credentials to your Azure function – use Shared Access Signature (SAS) tokens to achieve this task. Where possible, prefer to use client certificates to authenticate clients to your Azure functions. To allow your Azure functions access to Azure resources, use a system-assigned managed identity from Azure AD. If you need to store sensitive data (such as credentials), use Azure Key Vault. For sensitive environments, encrypt the Azure Functions application settings using customer-managed key management inside Azure Key Vault. Data and network access  Azure Functions can access resources in your Azure subscription – It is important to plan before deploying an Azure function. Azure Functions networking options Secure an HTTP endpoint in production IP address restrictions Azure Functions and FTP Protect your web apps and APIs Best practices  For better protection of your Azure functions, configure the Azure function behind Azure API Gateway. Use TLS 1.2 to encrypt sensitive data over the network. Create a separate Azure storage account for any newly created Azure function. Use Azure network security groups to block outbound traffic from your Azure functions (when internet access is not required). Use the Azure VNet service endpoint to control access to your Azure functions. Use Azure App Service static IP restrictions to control access to your Azure functions. Use either an Azure App Service Standard plan or an Azure App Service Premium plan to configure network isolations of your Azure functions. Use Azure Defender for App Service as an extra layer of protection for your Azure functions that have inbound access from the internet. Use Azure Web Application Firewall as an extra layer of protection for your Azure functions that have inbound access from the internet. Disable and block the use of the FTP protocol with your Azure functions. Conducting auditing and monitoring  Azure allows you to enable logging and auditing using the Azure Monitor service. Logging and threat detection Azure security baseline for Azure Functions Best practices  Use the Azure Monitor service to log authentication-related activities of your Azure functions. Use the Security Center threat detection capability (Azure Defender). Conducting compliance, configuration change, and secure coding  Serverless, or function as a service, is mainly code running inside a closed, managed environment. As a customer, you cannot control the underlying infrastructure => invest in secure coding to avoid attackers breaking into your application and causing harm that Azure cannot protect against. Azure security baseline for Azure Functions Posture and vulnerability management OWASP Serverless Top 10 Best practices  Follow the OWASP Serverless Top 10 project documentation when writing your Azure Functions code. Use the Azure security baseline for Azure Functions (Azure Security Benchmark). Use the Security Center threat detection capability (Azure Defender).",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "3581a8cb7c7b91991b88b7ac45e60952",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/ebs",
    "t": "Elastic Block Store (EBS) ",
    "c": "Elastic Block Store (EBS)  Amazon Elastic Block Store (Amazon EBS) is the AWS block storage. When working with EC2 instances, it is common practices to attach an additional volume to store data (separately from the operating system volume). Amazon EBS can be attached to a single EC2 instance and can be accessed from within the operating system. The traffic between the EC2 instance and the attached EBS volume is encrypted at transit (and is automatically configured and controlled by AWS). And, an EBS volume can be configured to encrypt data at rest for the rare scenario in which a potential attacker gains access to your EBS volume and wishes to access your data. The data itself (on the EBS volume and its snapshots) is only accessible by the EC2 instance that is connected to the EBS volume. To create an encrypted EBS volume in a specific AWS availability zone: aws ec2 create-volume \\ --size 80 \\ --encrypted \\ --availability-zone <AWS_AZ_code> To enable EBS encryption by default in a specific AWS region: aws ec2 enable-ebs-encryption-by-default --region <Region_Code> Best practices  Configure encryption by default for each region you are planning to deploy EC2 instances. Encrypt both boot and data volumes. Encrypt each EBS volume at creation time. Encrypt EBS volume snapshots. Use AWS Config to detect unattached EBS volumes. Use an IAM policy to define who can attach, detach, or create a snapshot for EBS volumes to minimise the risk of data exfiltration. Avoid configuring public access to your EBS volume snapshots – make sure all snapshots are encrypted. For highly sensitive environments, encrypt EBS volumes using the customer master key. Set names and descriptions for EBS volumes to better understand which EBS volume belongs to which EC2 instance. Use tagging (labelling) for EBS volumes to allow a better understanding of which EBS volume belongs to which EC2 instance.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "2ae837501401c178925309a6f8f318e0",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/sql-mysql",
    "t": "SQL for MySQL ",
    "c": "SQL for MySQL  Configuring IAM  MySQL supports the following types of authentication methods: Local username/password authentication against the MySQL built-in authentication mechanism . Google Cloud IAM authentication . Best practices  For the local MySQL default user, create a strong and complex password, and keep the password in a secured location. For end users who need direct access to the managed database, the preferred method is to use Google Cloud IAM authentication. Network access  Access to a managed MySQL database service is controlled via one of the following options: Authorised networks : Allows you to configure which IP addresses (or CIDR) are allowed to access your managed MySQL database. Cloud SQL Auth proxy : Client installed on your application side, which handles authentication to the Cloud SQL for MySQL database in a secure and encrypted tunnel. Best practices  Managed databases must never be accessible from the internet or a publicly accessible subnet – always use private subnets to deploy your databases. If possible, the preferred option is to use the Cloud SQL Auth proxy . Configure authorised networks for your web or application servers to allow access to your Cloud SQL for MySQL. If you need to manage the MySQL database service, use either a GCE VM instance to manage the MySQL database remotely or a Cloud VPN (configures an IPSec tunnel to a VPN gateway device). Stored data  To protect customers’ data, encrypt data both in transport and at rest. Best practices  Enforce TLS 1.2 transport layer encryption on your database. For sensitive environments, encrypt data at rest using customer-managed encryption keys (CMEKs) stored inside the Google Cloud KMS service. When using CMEKs , create a dedicated service account, and grant permission to the customers to access the encryption keys inside Google Cloud KMS. Enable auditing on all activities related to encryption keys. Conducting auditing and monitoring  As with any other managed service, GCP allows for logging and auditing using Google Cloud Audit Logs . Best practices  Admin activity audit logs are enabled by default and cannot be disabled. Explicitly enable data access audit logs to log activities performed on the database. Limit the access to audit logs to the minimum number of employees to avoid possible deletion or changes to the audit logs.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "837b90e6556987ab404444723b9a7107",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/csi",
    "t": "Container Storage Interface (CSI) ",
    "c": "Container Storage Interface (CSI)  Azure Kubernetes Service (AKS) has a CSI driver for the following storage types: Block storage: Azure Disk Managed SMB and NFS: Azure Files Best practices  Always use the latest CSI version for your chosen storage type. Use Azure Key Vault with the secret store CSI driver to store and retrieve secrets (such as tokens, SSH authentication keys, Docker configuration files, and more) to/from AKS pods. Use a private endpoint to connect your AKS cluster to Azure Files.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "c80e4662a2b55eeffca412b23722eee4",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/vpn",
    "t": "Securing VPN services ",
    "c": "Securing VPN services  Each cloud provider has its own implementation of a VPN service. VPNs allow network-based access to private resources over untrusted networks. Combined with a firewall, a VPN allows organizations to access and manage their internal resources (both sides of the VPN tunnel) in a secure manner. A VPN allows corporate users to connect to their organization’s cloud environment from either the corporate network or from home. The connection between the VPN and the cloud environment is encrypted. The connection to the cloud environment is transparent (that is, the same as working locally from the corporate network). The VPN can enforce the use of multifactor authentication (MFA) for end users connecting using a client VPN. Best practices  AWS Site-to-Site VPN AWS Client VPN Azure VPN Gateway (Site-to-Site) Azure VPN Gateway (Point-to-Site) Google Cloud VPN",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "18adfcf0de8decc7cc492253905a8fd4",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/iam",
    "t": "AWS IAM ",
    "c": "AWS IAM  AWS IAM terminology:  IAM user: This is a person or application with permission to access AWS resources. An IAM user has credentials (such as a password, access keys, and MFA). IAM group: This refers to a group of IAM users to make the permissions management task easier. IAM role: This indicates an identity that has permission to access resources without any credentials. Usually, you assign an IAM role to an IAM group, IAM user, or a service account that requires temporary permissions. Service account: This refers to a special type of IAM user, and its purpose is to allow applications to have access to resources. IAM policy: This is a JSON-based definition that sets the permissions for accessing AWS resources. There are two types of IAM policies: Identity-based policies: This is attached to a user, group, or role. Resource-based policies: This is attached to the AWS resource (for example, the Amazon S3 bucket). Identity provider: This refers to the ability to manage identities outside of AWS while still being able to grant access to AWS resources to the external identities (such as a federation between AWS and Azure AD). AWS IAM policy evaluation logic  Identity-based policies with resource-based policies: The result of both policies is the total permissions for both policies. Identity-based policies with permissions boundaries: The result of identity-based policies with the restrictions from permissions boundaries becomes the effective permissions. Identity-based policies with AWS Organizations service control policies: The result of both policies (for the account member of the organization) becomes the effective permissions. Best practices securing AWS IAM  Disable and remove all access keys and secret access keys from the AWS account root user. Configure a strong password for the AWS account root user. Enable MFA for the AWS account root user. Avoid using the AWS account root user. Instead, create a random password for the AWS root account, and in the rare scenarios where a root account is required, reset the root account password. Create an IAM user with a full IAM admin role to only the AWS console, to manage the AWS account. Enable MFA for any IAM user with a full IAM admin role. Avoid creating access keys and secret access keys for an IAM user with a full IAM admin role. Create IAM users with IAM policies according to the principle of least privilege. Create IAM groups, assign permissions to the groups, and add IAM users to those groups for easier account and permission management. For limited user access, create custom-managed policies and assign the custom policies to a group of users. Configure a password policy (which includes the minimum and maximum password age, the minimum password length, and enforces the use of password history and complex passwords). Use IAM roles to allow applications that are deploying on EC2 instances access to AWS resources. Use IAM roles instead of creating access keys and secret access keys to allow temporary access to resources rather than permanent access. Avoid embedding access keys and secret access keys inside your code. Instead, use secret management solutions such as AWS Secrets Manager to store and retrieve access keys. Rotate access keys periodically to avoid key abuse by malicious internal or external users. For highly sensitive environments or resources, set an IAM policy to restrict access based on conditions such as data, time, MFA, and more. For highly sensitive environments or resources, set an IAM policy to only allow access to users with MFA enabled. Use the AWS IAM AssumeRole capability to switch between multiple AWS accounts using the same IAM identity. Use IAM permissions boundaries to restrict IAM user access to specific AWS resources. Best practices auditing AWS IAM  Enable AWS CloudTrail on all AWS regions. Limit the level of access to the CloudTrail logs to a minimum number of employees – preferably to those with an AWS management account, outside the scope of your end users (including outside the scope of your IAM users), to avoid possible deletion or changes to the audit logs. Use Amazon GuardDuty to audit the AWS account root user activity. Use AWS CloudTrail to audit IAM user activities. Use IAM credential reports to locate users that haven’t logged in for a long period of time. Use an IAM policy simulator to check what the effective permissions for a specific IAM user are (that is, whether they are allowed or denied access to resources). Use AWS IAM Access Analyzer to detect unused permissions (from both your AWS account and cross-accounts) and fine-tune the permissions to your AWS resources. Use AWS IAM Access Analyzer to detect secrets that can be accessed from outside your AWS account and are stored inside the AWS Secrets Manager service. Use the IAMCTL tool to compare IAM policies between IAM user accounts to detect any changes (for example, by comparing template IAM user policies with one of your other IAM users).",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "232f035eb335d218f3905b5492542543",
    "u": "https://devsecops.tymyrddin.dev/docs/aws/ecs",
    "t": "Elastic Container Service (ECS) ",
    "c": "Elastic Container Service (ECS)  ECS is the Amazon-managed container orchestration service. It can integrate with other AWS services such as Amazon Elastic Container Registry (ECR) for storing containers, AWS IAM for managing permissions to ECS, and Amazon CloudWatch for monitoring ECS. Configuring IAM  AWS IAM is the supported service for managing permissions to access and run containers through Amazon ECS. Amazon ECS container instance IAM role . IAM roles for tasks . authorisation based on Amazon ECS tags . Using IAM to control filesystem data access . Amazon ECS task and container security . Best practices  Grant minimal IAM permissions for the Amazon ECS service. If you are managing multiple AWS accounts, use temporary credentials (using AWS Security Token Service or the AssumeRole capability) to manage ECS on the target AWS account with credentials from a source AWS account. Use service roles to allow the ECS service to assume your role and access resources such as S3 buckets, RDS databases, and so on. Use IAM roles to control access to Amazon Elastic File System (EFS) from ECS. Enforce multifactor authentication (MFA) for end users who have access to the AWS console and perform privileged actions such as managing the ECS service. Enforce policy conditions such as requiring end users to connect to the ECS service using a secured channel (SSL/TLS), connecting using MFA, log in at specific hours of the day, etc. Store your container images inside Amazon ECR and grant minimal IAM permissions for accessing and managing Amazon ECR. Network access  Since Amazon ECS is a managed service, it is located outside the customer’s VPC. An alternative to secure access from a VPC to the managed ECS environment is to use AWS PrivateLink , which avoids sending network traffic outside your VPC, through a secure channel, using an interface VPC endpoint. Best practices  Use a secured channel (TLS 1.2) to control Amazon ECS using API calls. Use VPC security groups to allow access from your VPC to the Amazon ECS VPC endpoint. If you use AWS Secrets Manager to store sensitive data (such as credentials) from Amazon ECS, use a Secrets Manager VPC endpoint when configuring security groups. If you use AWS Systems Manager to remotely execute commands on Amazon ECS, use Systems Manager VPC endpoints when configuring security groups. Store container images inside Amazon ECR and for non-sensitive environments, encrypt your container images inside Amazon ECR using AWS KMS. For sensitive environments, encrypt container images inside Amazon ECR using CMK management. If you use Amazon ECR to store container images, use VPC security groups to allow access from your VPC to the Amazon ECR interface’s VPC endpoint. Conducting auditing and monitoring  As with any other managed service, AWS allows you to enable logging and auditing using Amazon CloudWatch and AWS CloudTrail: Logging and monitoring in Amazon ECS . Logging Amazon ECS API calls with AWS CloudTrail . Best practices  Enable Amazon CloudWatch alarms for high-performance usage (which may indicate an anomaly in the ECS cluster behaviour). Enable AWS CloudTrail for any action performed on the ECS cluster. Limit the access to the CloudTrail logs to the minimum number of employees – preferably in an AWS management account, outside the scope of your end users (including outside the scope of your ECS cluster administrators), to avoid possible deletion or changes to the audit logs. Enabling compliance  Best practices  Use only trusted image containers and store them inside Amazon ECR – a private repository for storing your organisational images. Run the Docker Bench for Security tool on a regular basis to check for compliance with CIS Benchmarks for Docker containers. Build your container images from scratch (to avoid malicious code in preconfigured third-party images). Scan your container images for vulnerabilities in libraries and binaries and update your images on a regular basis. Configure your images with a read-only root filesystem to avoid unintended upload of malicious code into your images.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "f613416cfbd70ffdc2a29d79b652e217",
    "u": "https://devsecops.tymyrddin.dev/docs/azure/disks",
    "t": "Managed disks ",
    "c": "Managed disks  Azure-managed disks are Azure managed block level storage. It is common when working with VMs, to attach an additional volume to store data (separately from the operating system volume). This is also known as block storage. To encrypt a specific VM, in a specific resource group, using a unique customer key vault: az vm encryption enable -g MyResourceGroup --name MyVM --disk- encryption-keyvault myKV To show the encryption status of a specific VM in a specific resource group: az vm encryption show --name \"myVM\" -g \"MyResourceGroup\" Best practices  Create encryption keys (inside the Azure Key Vault service) for each region you are planning to deploy VMs in. For Windows machines, encrypt your data using BitLocker technology. For Linux machines, encrypt your data using dm-crypt technology. Encrypt both the OS and data volumes. Encrypt each data volume at creation time. Encrypt the VM snapshots. Use an Azure private link service to restrict the export and import of managed disks to the Azure network. For highly sensitive environments, encrypt data volumes using a CMK. Set names for the Azure disk volumes to allow a better understanding of which disk volume belongs to which VM. Use tagging (labelling) for disk volumes to allow a better understanding of which disk volume belongs to which VM.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "27c7db591ef4583b218334c43050f4c9",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/object",
    "t": "Securing object storage ",
    "c": "Securing object storage  Each cloud provider has its own implementation of object storage. The basic idea is the same: Object storage is a special type of storage that is meant to store data. Files (or objects) are stored inside buckets (these are logical concepts such as directories or logical containers). Access to files on object storage is either done through the HTTP(S) protocol API via web command-line tools or programmatically using SDK tools. Object storage is not meant to store operating systems or databases (please refer to the Securing block storage section). Best practices  Amazon Simple Storage Service Azure Blob storage Google Cloud Storage",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "71a60967bc07afea7044e72526f4cd04",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/filestore",
    "t": "Filestore ",
    "c": "Filestore  Google Filestore is a GCP file storage service based on the NFS protocol. Authentication and authorisation  Google Cloud IAM is the supported service in which to manage permissions to access Google Filestore. Security for server client libraries Get started with Cloud Firestore Security Rules Writing conditions for Cloud Firestore Security Rules Best practices  Keep your Google Filestore instances private. Create an IAM group, add users to the IAM group, and then grant the required permissions on the target Google Filestore instance to the target IAM group. Use IAM roles to configure minimal permissions to any Google Filestore instance. Use Cloud Firestore Security Rules to allow mobile clients, web clients, or serverless authentication to Google Filestore. Network access  Google Filestore is a managed service, and is located outside the customer’s VPC. Protect access to Google Filestore. Architecture Access Control Configuring IP-based access control Configuring Firewall Rules Best practices  Use IP-based access control to restrict access to Google Filestore. Create a Google Filestore instance on the same VPC as your clients. If the Google Filestore instance is located outside your VPC, use VPC firewall rules to restrict access between your VPC and Google Filestore.",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "dce0db0ea4dd782867e41b2c26620655",
    "u": "https://devsecops.tymyrddin.dev/docs/notes/vms",
    "t": "Securing virtual machines ",
    "c": "Securing virtual machines  Each cloud provider has its own implementation of VMs (or virtual servers). The basic idea remains the same: Select a machine type (or size). Select a preinstalled image of an operating system. Configure storage. Configure network settings. Configure permissions to access cloud resources. Deploy an application. Use the service. Carry out ongoing maintenance of the operating system. With the shared responsibility model , when using IaaS, we (as the customers) are responsible for the deployment and maintenance of virtual servers. Best practices  Elastic Compute Cloud (EC2) Azure Virtual Machines Google Compute Engine (GCE) and VM instances",
    "cat": "blue_dev",
    "type": "html"
  },
  {
    "objectID": "303ca0dd49fae8caf560f55e34ed9e26",
    "u": "https://devsecops.tymyrddin.dev/docs/gcp/iam",
    "t": "Google Cloud IAM ",
    "c": "Google Cloud IAM  Google Cloud IAM terminology  Member: This is a Google account (for example, a user), a service account (for applications and virtual machines), or Cloud Identity with access to GCP resources. User: This is a person or application (a service account) with permissions to access GCP resources. A user has credentials (such as a password and MFA). Group: This is a group of users to make the permissions management task easier. Role: This refers to a collection of permissions to access resources. Service account: This is a special type of IAM user to allow applications access to resources. IAM policy: This is a JSON-based definition that sets the permissions for accessing GCP resources. GCP policy evaluation  No organization policy set: The default access to resources is enforced. Inheritance: If a resource node has set inheritFromParent = true, then the effective policy of the parent resource is inherited. Disallow inheritance: If a resource hierarchy node has a policy that includes inheritFromParent = false, it doesn’t inherit the organization policy from its parent. Reconciling policy conflicts: By default, policies are inherited and merged; but DENY values always take precedence. Best practices securing cloud IAM  Use Google Workspace to create and manage user accounts. Configure a password policy (which includes the minimum and maximum password age, the minimum password length, and enforces the use of password history and complex passwords) from within your Google Workspace admin console. Enable MFA for any user with high privileges in your Google Cloud (such as the GCP project owner role). Create IAM groups, assign permissions to the groups, and add users to those groups for easier account and permission management. Use service accounts to grant applications minimal access to Google Cloud resources. Create a dedicated service account for each application. For scenarios where you only need an application to access resources for a short amount of time, use short-lived service account credentials. Disable unused service accounts. Use Google Managed Service accounts for services (such as Google Compute Engine) that require access to Google resources (such as Google Cloud Storage). Use role recommendations using IAM Recommender to enforce minimal permissions to Google resources. Limit the use of service account keys, or avoid them completely, to avoid having to expose access to your Google Cloud resources from outside your GCP environment. Use IAM conditions to enforce the location from which a user can access resources in your Google Cloud environment (such as your office network). Use the policy simulator to determine the effect of a policy on your users. Best practices auditing cloud IAM  Enable Cloud IAM audit logs for further log analysis, such as tracking failed Active Directory logins. Admin activity audit logs are enabled by default and cannot be disabled. Explicitly enable Data access audit logs to log activities performed on Google Cloud IAM. Limit the level of access to the audit logs to a minimum number of employees, to avoid possible deletion or changes to the audit logs using IAM roles. Use Policy Analyzer to determine which identity (for example, a user or a service account) has access to which Google Cloud resource and the type of access. Use service account insights to identify unused service accounts and service account keys.",
    "cat": "blue_dev",
    "type": "html"
  }
]